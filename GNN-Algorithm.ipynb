{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11324692,"sourceType":"datasetVersion","datasetId":6805762}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"####  All the imports  ####\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport os\nimport logging\nimport time\nfrom scipy.sparse import csr_matrix\n\nimport networkx as nx\nimport pickle\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# file_path = '/kaggle/input/recohit-dataset-100/RecoOutPileup_TimeMod_uniform_1_100recohitfile.tsv'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model_path = \"/kaggle/input/new-model-4dim/trained_full_model.pt\"\n\nimport networkx as nx\nfrom joblib import Parallel, delayed\n\nimport torch.optim as optim\n\nfrom torch.utils.data import random_split\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfolder_path = '/kaggle/working/embed_data'\nif not os.path.exists(folder_path):\n    os.makedirs(folder_path)\n    \nmodel_dir = \"/kaggle/working/model\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n# Save the figures in the \"plots\" folder\nplots_dir = \"/kaggle/working/plots\"\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import minmax_scale\n\n# test_embedded_points\nimport os\n\n# Check if the directory already exists.\nif not os.path.exists(\"/kaggle/working/experiment_dir\"):\n\n    os.makedirs(\"/kaggle/working/experiment_dir\")\nexperiment_dir = \"/kaggle/working/experiment_dir\"\n\n# Create the \"embed_data\" folder if it doesn't exist\nfolder_path = '/kaggle/working/embed_data'\nif not os.path.exists(folder_path):\n    os.makedirs(folder_path)\n    \n# model_path = '/kaggle/input/model-new-4d/trained_full_model.pt'\n# if not os.path.exists(model_path):\n#     os.makedirs(model_path)\n    \n# Save the figures in the \"plots\" folder\nplots_dir = \"/kaggle/working/plots\"\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n    \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nexperiment_dir = '/kaggle/working/'\n\nimport torch\n# from torch_geometric.nn import GCNConv\nimport networkx as nx\nimport numpy as np\nfrom sklearn.neighbors import KDTree\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.cluster import DBSCAN\nfrom collections import defaultdict\nimport time\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom concurrent.futures import ThreadPoolExecutor\nimport numpy as np\nimport networkx as nx\nfrom collections import defaultdict\nimport pickle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch\nimport torch.nn as nn\nimport time\nimport os\nimport logging\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.data import DataLoader\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#Import all the libraries\n#Install trackml, run these 3 seperately\n\n\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#defining the class\n\n\n#####################################\n#               DATASET             #\n#####################################\n\nclass Hit_Pair_Dataset(Dataset):\n    def __init__(self, data_filepath, nb_samples):\n        dataset = pd.read_csv(\"/kaggle/input/new-set/pairwise_data_normalized.csv\")\n\n        try:\n            self.hits_a = np.array(dataset[['pos_x1', 'pos_y1', 'pos_z1', 'time_ns1']][:nb_samples], dtype=np.float32)\n            self.hits_b = np.array(dataset[['pos_x2', 'pos_y2', 'pos_z2', 'time_ns2']][:nb_samples], dtype=np.float32)\n            self.target = np.array(dataset['label'][:nb_samples], dtype=np.float32)\n            self.time1 = np.array(dataset['time_ns1'][:nb_samples], dtype=np.float32)\n            self.muon1 = np.array(dataset['track_no1'][:nb_samples], dtype=np.float32)\n        \n        except:\n            dim = (dataset.shape[1] - 1) // 2\n            self.hits_a = dataset[:nb_samples, :dim]\n            self.hits_b = dataset[:nb_samples, dim:2 * dim]\n            self.target = dataset[:nb_samples]\n            self.time1 = dataset[:nb_samples]\n            self.muon1 = dataset[:nb_samples]\n            \n\n    def __getitem__(self, index):\n        h_a = self.hits_a[index]\n        h_b = self.hits_b[index]\n        t = self.target[index]\n        ti = self.time1[index]\n        mi = self.muon1[index]\n        return h_a, h_b, t, ti, mi\n\n    def __len__(self):\n        return len(self.hits_a)\n\n    def get_dim(self):\n        return self.hits_a.shape[1]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndataset = Hit_Pair_Dataset(\"/kaggle/input/new-set/pairwise_data_normalized.csv\", nb_samples=30000)\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 nb_hidden,\n                 nb_layer,\n                 input_dim,\n                 mean,\n                 std,\n                 emb_dim=4):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, nb_hidden)]\n        ln = [nn.Linear(nb_hidden, nb_hidden) for _ in range(nb_layer-1)]\n        layers.extend(ln)\n        self.layers = nn.ModuleList(layers)\n        self.emb_layer = nn.Linear(nb_hidden, emb_dim)\n        self.act = nn.ReLU()\n        self.mean = torch.FloatTensor(mean).to(torch.float)\n        self.std = torch.FloatTensor(std).to(torch.float)\n\n    def forward(self, hits):\n        hits = self.normalize(hits)\n        for l in self.layers:\n            hits = l(hits)\n            hits = self.act(hits)\n            # hits = self.dropout(hits)\n        hits = self.emb_layer(hits)\n        return hits\n\n    def normalize(self, hits):\n\n        return hits\n\nmlp = MLP(nb_hidden=128, nb_layer=3, input_dim=4, mean=[0,0,0,0], std=[1,1,1,1])\nprint(\"MLP model:\", mlp)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#score calculation: correctly predicted labels/total labels\n\ndef score_dist_accuracy(pred, true):\n    pred_classes = torch.ones_like(pred)  # Initialize all predictions as 1\n    pred_classes[pred >= 0.1] = -1 \n    correct = pred_classes == true\n    nb_correct = correct.sum().item()\n    nb_total = true.size(0)\n    score = nb_correct / nb_total\n    return score\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n####################\n#     PRINTING     #\n####################\ndef print_header():\n  '''\n  Print header before train / evaluation run.\n  '''\n  logging.info(\"         Loss  Score\")\n\ndef print_eval_stats(nb_processed, loss, score):\n  '''\n  Log stats during train, evaluation run.\n  '''\n  logging.info(\"  {:5d}: {:.3f}  {:2.2f}\".format(nb_processed, loss, score))\n    \n#spliting the dataset for training and evaluation\n\nfrom torch.utils.data import random_split\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the dataset into training and validation subsets\ntrain_ratio = 0.8\nvalid_ratio = 0.2\ntrain_size = int(train_ratio * len(dataset))\nvalid_size = len(dataset) - train_size\n\n# train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n\ntrain_dataset, valid_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n\n\n# Create data loaders for training and validation\nbatch_size = 256\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n# Check if a GPU is available, otherwise use CPU\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ninput_dim = dataset.get_dim()\nnet = MLP(nb_hidden=128, nb_layer=3, input_dim=4, mean=[0,0,0,0], std=[1,1,1,1])\nnet.to(DEVICE)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#defining the model for training one epoch\n\n\nmodel_dir = \"/kaggle/working/model\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n\ndef train_one_epoch(net, batch_size, optimizer, train_loader):\n  net.train()\n\n  nb_batch = len(train_loader)\n  nb_train = nb_batch * batch_size\n  epoch_score = 0\n  epoch_loss  = 0\n\n\n  logging.info(\"Training on {} samples\".format(nb_train))\n  print_header()\n  t0 = time.time()\n  elapsed = 0\n  for i, (hits_a, hits_b, target, time1, muon1) in enumerate(train_loader):\n    hits_a = hits_a.to(DEVICE, non_blocking=True)\n    hits_b = hits_b.to(DEVICE, non_blocking=True)\n    target = target.to(DEVICE, non_blocking=True)\n    time1 = time1.to(DEVICE, non_blocking=True)\n    muon1 = muon1.to(DEVICE, non_blocking=True)\n    '''\n    hits_a = hits_a.to(DEVICE)\n    hits_b = hits_b.to(DEVICE)\n    target = target.to(DEVICE)\n    '''\n    optimizer.zero_grad()\n\n    emb_h_a = net(hits_a)\n    emb_h_b = net(hits_b)\n\n    pred = nn.functional.pairwise_distance(emb_h_a,emb_h_b)\n    true_dist = target\n    loss = nn.functional.hinge_embedding_loss(pred,true_dist)\n    \n    torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n\n    loss.backward()\n    optimizer.step()\n\n    score = score_dist_accuracy(pred, target)\n    epoch_score += score * 100\n    epoch_loss  += loss.item()\n\n    nb_proc = (i+1) * batch_size\n    if (((i+0) % (nb_batch//10)) == 0):\n        print_eval_stats(nb_proc,\n                               epoch_loss/(i+1),\n                               epoch_score/(i+1))\n\n  logging.info(\"Model elapsed:  {:.2f}\".format(elapsed))\n\n  # Save the trained model\n  torch.save(net.state_dict(), os.path.join(model_dir, \"trained_model.pt\"))\n\n  return epoch_loss / nb_batch, epoch_score / nb_batch\n\n#   Save the trained model\n# torch.save(net.state_dict(), os.path.join(model_dir, \"trained_model.pt\"))\n\n\ndef evaluate(net, batch_size, valid_loader):\n    net.eval()\n\n    nb_batch = len(valid_loader)\n    nb_valid = nb_batch * batch_size\n    valid_score = 0\n    valid_loss = 0\n\n    logging.info(\"Evaluating on {} samples\".format(nb_valid))\n    print_header()\n    t0 = time.time()\n    elapsed = 0\n    with torch.no_grad():\n        for i, (hits_a, hits_b, target, time1, muon1) in enumerate(valid_loader):\n            hits_a = hits_a.to(DEVICE, non_blocking=True)\n            hits_b = hits_b.to(DEVICE, non_blocking=True)\n            target = target.to(DEVICE, non_blocking=True)\n            time1 = time1.to(DEVICE, non_blocking=True)\n            muon1 = muon1.to(DEVICE, non_blocking=True)\n\n            emb_h_a = net(hits_a)\n            emb_h_b = net(hits_b)\n\n            pred = nn.functional.pairwise_distance(emb_h_a, emb_h_b)\n            true_dist = target\n            loss = nn.functional.hinge_embedding_loss(pred, true_dist)\n\n            score = score_dist_accuracy(pred, target)\n            valid_score += score * 100\n            valid_loss += loss.item()\n\n            nb_proc = (i + 1) * batch_size\n            if (((i + 0) % (nb_batch // 10)) == 0):\n                print_eval_stats(nb_proc, valid_loss / (i + 1), valid_score / (i + 1))\n                \n            # Save the evaluated model\n            torch.save(net.state_dict(), os.path.join(model_dir, \"evaluated_model.pt\"))\n\n    logging.info(\"Model elapsed:  {:.2f}\".format(elapsed))\n\n    return valid_loss / nb_batch, valid_score / nb_batch\n#   Save the evaluated model\n\nbatch_size = 512\n\nimport torch.optim as optim\n\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnet.to(DEVICE)\n\n# Define the optimizer\noptimizer = optim.Adam(net.parameters(), lr=0.0001)\n# optimizer = optim.SGD(net.parameters(), lr=0.001)\n\n\n# Train the model\ntrain_losses=[]\ntrain_scores=[]\nvalid_losses = []\nvalid_scores = []\n\nnum_epochs = 35\nfor epoch in range(num_epochs):\n    epoch_loss, epoch_score = train_one_epoch(net, batch_size, optimizer, train_loader)\n    train_losses.append(epoch_loss)\n    train_scores.append(epoch_score)\n    valid_loss, valid_score = evaluate(net, batch_size, valid_loader)\n    valid_losses.append(valid_loss)\n    valid_scores.append(valid_score)\n#     print(f\"Epoch {epoch+1}: Loss: {epoch_loss}, Score: {epoch_score}\")\n    print(f\"Epoch {epoch+1}: Train Loss: {epoch_loss}, Train Score: {epoch_score}, Valid Loss: {valid_loss}, Valid Score: {valid_score}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Plot train loss and valid loss in one plot\n\n# Save the figures in the \"plots\" folder\nplots_dir = \"/kaggle/working/plots\"\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Save the Train Loss vs. Valid Loss plot\nloss_plot_filename = os.path.join(plots_dir, \"train_valid_loss_plot.png\")\n\n#Plot loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\nplt.plot(range(1, num_epochs + 1), valid_losses, label='Valid Loss', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train Loss vs. Valid Loss')\nplt.legend()\nplt.grid()\nplt.savefig(loss_plot_filename)\nplt.show()\n\n\n# Save the Train Accuracy vs. Valid Accuracy plot\naccuracy_plot_filename = os.path.join(plots_dir, \"train_valid_accuracy_plot.png\")\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_scores, label='Train Accuracy', marker='o')\nplt.plot(range(1, num_epochs + 1), valid_scores, label='Valid Accuracy', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train Accuracy vs. Valid Accuracy')\nplt.legend()\nplt.grid()\nplt.savefig(accuracy_plot_filename)\n# Close the figure to free up memory\nplt.show()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#defining all the necessary functions\n\ndef track_epoch_stats(epoch_nb, lrate, train_stats, val_stats, experiment_dir):\n    print(\"Epoch: {}\".format(epoch_nb))\n    print(\"Learning rate: {:.3g}\".format(lrate))\n    print(\"Train loss: {:.3f}\".format(train_stats[0]))\n    print(\"Train score: {:.2f}\".format(train_stats[1]))\n    print(\"Validation loss: {:.3f}\".format(val_stats[0]))\n    print(\"Validation score: {:.2f}\".format(val_stats[1]))\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch\nimport torch.nn as nn\nimport time\nimport os\nimport logging\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.data import DataLoader\n\n\nimport os\n\n\n\n\n#####################################\n#               DATASET             #\n#####################################\n\nclass Hit_Pair_Dataset(Dataset):\n    def __init__(self, data_filepath, nb_samples):\n        dataset = pd.read_csv(\"/kaggle/input/new-set/pairwise_data_normalized.csv\")\n\n        try:\n            self.hits_a = np.array(dataset[['pos_x1', 'pos_y1', 'pos_z1', 'time_ns1']][:nb_samples], dtype=np.float32)\n            self.hits_b = np.array(dataset[['pos_x2', 'pos_y2', 'pos_z2', 'time_ns2']][:nb_samples], dtype=np.float32)\n            self.target = np.array(dataset['label'][:nb_samples], dtype=np.float32)\n            \n            self.muon1 = np.array(dataset['track_no1'][:nb_samples], dtype=np.float32)\n        \n        except:\n            dim = (dataset.shape[1] - 1) // 2\n            self.hits_a = dataset[:nb_samples, :dim]\n            self.hits_b = dataset[:nb_samples, dim:2 * dim]\n            self.target = dataset[:nb_samples]\n            \n            self.muon1 = dataset[:nb_samples]\n            \n\n    def __getitem__(self, index):\n        h_a = self.hits_a[index]\n        h_b = self.hits_b[index]\n        t = self.target[index]\n        \n        mi = self.muon1[index]\n        return h_a, h_b, t, mi\n\n    def __len__(self):\n        return len(self.hits_a)\n\n    def get_dim(self):\n        return self.hits_a.shape[1]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndataset = Hit_Pair_Dataset(\"/kaggle/input/new-set/pairwise_data_normalized.csv\", nb_samples=30000)\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 nb_hidden,\n                 nb_layer,\n                 input_dim,\n                 mean,\n                 std,\n                 emb_dim=4):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, nb_hidden)]\n        ln = [nn.Linear(nb_hidden, nb_hidden) for _ in range(nb_layer-1)]\n        layers.extend(ln)\n        self.layers = nn.ModuleList(layers)\n        self.emb_layer = nn.Linear(nb_hidden, emb_dim)\n        self.act = nn.ReLU()\n        self.mean = torch.FloatTensor(mean).to(torch.float)\n        self.std = torch.FloatTensor(std).to(torch.float)\n\n    def forward(self, hits):\n        hits = self.normalize(hits)\n        for l in self.layers:\n            hits = l(hits)\n            hits = self.act(hits)\n            # hits = self.dropout(hits)\n        hits = self.emb_layer(hits)\n        return hits\n\n    def normalize(self, hits):\n\n        return hits\n\n# mlp = MLP(nb_hidden=256, nb_layer=4, input_dim=4, mean=[0,0,0,0], std=[1,1,1,1])\n# print(\"MLP model:\", mlp)\n\n\ndef score_dist_accuracy(pred, true):\n    pred_classes = torch.ones_like(pred)  # Initialize all predictions as 1\n    pred_classes[pred >= 0.1] = -1 \n    correct = pred_classes == true\n    nb_correct = correct.sum().item()\n    nb_total = true.size(0)\n    score = nb_correct / nb_total\n    return score\n\n\n####################\n#     PRINTING     #\n####################\ndef print_header():\n  '''\n  Print header before train / evaluation run.\n  '''\n  logging.info(\"         Loss  Score\")\n\ndef print_eval_stats(nb_processed, loss, score):\n  '''\n  Log stats during train, evaluation run.\n  '''\n  logging.info(\"  {:5d}: {:.3f}  {:2.2f}\".format(nb_processed, loss, score))\n    \n#spliting the dataset for training and evaluation\n\nfrom torch.utils.data import random_split\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the dataset into training and validation subsets\ntrain_ratio = 0.8\nvalid_ratio = 0.2\ntrain_size = int(train_ratio * len(dataset))\nvalid_size = len(dataset) - train_size\n\n# train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n\ntrain_dataset, valid_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n\n\n# Create data loaders for training and validation\nbatch_size = 256\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n# Check if a GPU is available, otherwise use CPU\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ninput_dim = dataset.get_dim()\nnet = MLP(nb_hidden=64, nb_layer=3, input_dim=4, mean=[0,0,0,0], std=[1,1,1,1])\nnet.to(DEVICE)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n#defining the model for training one epoch\n\n\nmodel_dir = \"/kaggle/working/model\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n\ndef train_one_epoch(net, batch_size, optimizer, train_loader):\n  net.train()\n\n  nb_batch = len(train_loader)\n  nb_train = nb_batch * batch_size\n  epoch_score = 0\n  epoch_loss  = 0\n\n\n  logging.info(\"Training on {} samples\".format(nb_train))\n  print_header()\n  t0 = time.time()\n  elapsed = 0\n  for i, (hits_a, hits_b, target,  muon1) in enumerate(train_loader):\n    hits_a = hits_a.to(DEVICE, non_blocking=True)\n    hits_b = hits_b.to(DEVICE, non_blocking=True)\n    target = target.to(DEVICE, non_blocking=True)\n    \n    muon1 = muon1.to(DEVICE, non_blocking=True)\n    '''\n    hits_a = hits_a.to(DEVICE)\n    hits_b = hits_b.to(DEVICE)\n    target = target.to(DEVICE)\n    '''\n    optimizer.zero_grad()\n\n    emb_h_a = net(hits_a)\n    emb_h_b = net(hits_b)\n\n    pred = nn.functional.pairwise_distance(emb_h_a,emb_h_b)\n    true_dist = target\n    loss = nn.functional.hinge_embedding_loss(pred,true_dist)\n    \n    torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n\n    loss.backward()\n    optimizer.step()\n\n    score = score_dist_accuracy(pred, target)\n    epoch_score += score * 100\n    epoch_loss  += loss.item()\n\n    nb_proc = (i+1) * batch_size\n    if (((i+0) % (nb_batch//10)) == 0):\n        print_eval_stats(nb_proc,\n                               epoch_loss/(i+1),\n                               epoch_score/(i+1))\n\n  logging.info(\"Model elapsed:  {:.2f}\".format(elapsed))\n\n  # Save the trained model\n  torch.save(net.state_dict(), os.path.join(model_dir, \"trained_model1.pt\"))\n\n  return epoch_loss / nb_batch, epoch_score / nb_batch\n\n#   Save the trained model\n# torch.save(net.state_dict(), os.path.join(model_dir, \"trained_model.pt\"))\n\n\ndef evaluate(net, batch_size, valid_loader):\n    net.eval()\n\n    nb_batch = len(valid_loader)\n    nb_valid = nb_batch * batch_size\n    valid_score = 0\n    valid_loss = 0\n\n    logging.info(\"Evaluating on {} samples\".format(nb_valid))\n    print_header()\n    t0 = time.time()\n    elapsed = 0\n    with torch.no_grad():\n        for i, (hits_a, hits_b, target,  muon1) in enumerate(valid_loader):\n            hits_a = hits_a.to(DEVICE, non_blocking=True)\n            hits_b = hits_b.to(DEVICE, non_blocking=True)\n            target = target.to(DEVICE, non_blocking=True)\n            \n            muon1 = muon1.to(DEVICE, non_blocking=True)\n\n            emb_h_a = net(hits_a)\n            emb_h_b = net(hits_b)\n\n            pred = nn.functional.pairwise_distance(emb_h_a, emb_h_b)\n            true_dist = target\n            loss = nn.functional.hinge_embedding_loss(pred, true_dist)\n\n            score = score_dist_accuracy(pred, target)\n            valid_score += score * 100\n            valid_loss += loss.item()\n\n            nb_proc = (i + 1) * batch_size\n            if (((i + 0) % (nb_batch // 10)) == 0):\n                print_eval_stats(nb_proc, valid_loss / (i + 1), valid_score / (i + 1))\n                \n            # Save the evaluated model\n            torch.save(net.state_dict(), os.path.join(model_dir, \"evaluated_model1.pt\"))\n\n    logging.info(\"Model elapsed:  {:.2f}\".format(elapsed))\n\n    return valid_loss / nb_batch, valid_score / nb_batch\n#   Save the evaluated model\n\nbatch_size = 512\n\nimport torch.optim as optim\n\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnet.to(DEVICE)\n\n# Define the optimizer\noptimizer = optim.Adam(net.parameters(), lr=0.0001)\n# optimizer = optim.SGD(net.parameters(), lr=0.001)\n\n\n# Train the model\ntrain_losses=[]\ntrain_scores=[]\nvalid_losses = []\nvalid_scores = []\n\nnum_epochs = 35\nfor epoch in range(num_epochs):\n    epoch_loss, epoch_score = train_one_epoch(net, batch_size, optimizer, train_loader)\n    train_losses.append(epoch_loss)\n    train_scores.append(epoch_score)\n    valid_loss, valid_score = evaluate(net, batch_size, valid_loader)\n    valid_losses.append(valid_loss)\n    valid_scores.append(valid_score)\n#     print(f\"Epoch {epoch+1}: Loss: {epoch_loss}, Score: {epoch_score}\")\n    print(f\"Epoch {epoch+1}: Train Loss: {epoch_loss}, Train Score: {epoch_score}, Valid Loss: {valid_loss}, Valid Score: {valid_score}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Plot train loss and valid loss in one plot\n\n# Save the figures in the \"plots\" folder\nplots_dir = \"/kaggle/working/plots\"\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Save the Train Loss vs. Valid Loss plot\nloss_plot_filename = os.path.join(plots_dir, \"train_valid_loss_plot.png\")\n\n#Plot loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\nplt.plot(range(1, num_epochs + 1), valid_losses, label='Valid Loss', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train Loss vs. Valid Loss')\nplt.legend()\nplt.grid()\nplt.savefig(loss_plot_filename)\nplt.show()\n\n\n# Save the Train Accuracy vs. Valid Accuracy plot\naccuracy_plot_filename = os.path.join(plots_dir, \"train_valid_accuracy_plot.png\")\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_scores, label='Train Accuracy', marker='o')\nplt.plot(range(1, num_epochs + 1), valid_scores, label='Valid Accuracy', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train Accuracy vs. Valid Accuracy')\nplt.legend()\nplt.grid()\nplt.savefig(accuracy_plot_filename)\n# Close the figure to free up memory\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########################################\n#    Query ball based algorithm      #\n#######################################\n\nimport time as ti\nstart_time = ti.time()\nfrom scipy.sparse import csr_matrix\nfile_path1 = '/kaggle/input/new-set/clean_data_1000_low_new_sec.csv'\ndata = pd.read_csv(file_path1, delimiter = ',')\n\nmodel_path = \"/kaggle/input/new-set/trained_model1.pt\"\n# model_gnn = '/kaggle/input/model-gnn/gnn_model (1).pth'\n###   embedded space creation   ####\nchunk_size = 1000  \ndata_chunks = pd.read_csv(file_path1, chunksize=chunk_size, delimiter = ',')#, delimiter = '\\t'\n\n# Initialize an empty list to store chunks after processing\nprocessed_chunks = []\n\n# Process each chunk and assign unique HitID\n\nfor chunk in data_chunks:\n    # Filter rows with muonID not equal to -999\n#     chunk = chunk[chunk['muonID'] != -999]\n\n    if not chunk.empty:\n        # Generate unique HitID based on index\n        chunk['HitID'] = chunk.index + 1\n\n        # Append processed chunk to the list\n        processed_chunks.append(chunk)\n\ndata = pd.concat(processed_chunks, ignore_index=True)\n\ndata.to_csv(\"/kaggle/working/processed_data_with_hitid1.csv\", index=False)\n\nprint(data.head())\n\n####  All the classes and definitions  ####\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 nb_hidden,\n                 nb_layer,\n                 input_dim,\n                 mean,\n                 std,\n                 emb_dim=4):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, nb_hidden)]\n        ln = [nn.Linear(nb_hidden, nb_hidden) for _ in range(nb_layer-1)]\n        layers.extend(ln)\n        self.layers = nn.ModuleList(layers)\n        self.emb_layer = nn.Linear(nb_hidden, emb_dim)\n        self.act = nn.ReLU()\n        self.mean = torch.FloatTensor(mean).to(torch.float)\n        self.std = torch.FloatTensor(std).to(torch.float)\n\n    def forward(self, hits):\n        hits = self.normalize(hits)\n        for l in self.layers:\n            hits = l(hits)\n            hits = self.act(hits)\n            # hits = self.dropout(hits)\n        hits = self.emb_layer(hits)\n        return hits\n\n#     def normalize(self, hits):\n\n#         return hits\n    def normalize(self, hits):\n\n        return hits\n\n\n\n#score calculation: correctly predicted labels/total labels\n\ndef score_dist_accuracy(pred, true):\n    pred_classes = torch.ones_like(pred)  # Initialize all predictions as 1\n    pred_classes[pred >= 0.1] = -1 \n    correct = pred_classes == true\n    nb_correct = correct.sum().item()\n    nb_total = true.size(0)\n    score = nb_correct / nb_total\n    return score\n\n####################\n#     PRINTING     #\n####################\ndef print_header():\n  '''\n  Print header before train / evaluation run.\n  '''\n  logging.info(\"         Loss  Score\")\n\ndef print_eval_stats(nb_processed, loss, score):\n  '''\n  Log stats during train, evaluation run.\n  '''\n  logging.info(\"  {:5d}: {:.3f}  {:2.2f}\".format(nb_processed, loss, score))\n\n\n#defining all the necessary functions\n\ndef track_epoch_stats(epoch_nb, lrate, train_stats, val_stats, experiment_dir):\n    print(\"Epoch: {}\".format(epoch_nb))\n    print(\"Learning rate: {:.3g}\".format(lrate))\n    print(\"Train loss: {:.3f}\".format(train_stats[0]))\n    print(\"Train score: {:.2f}\".format(train_stats[1]))\n    print(\"Validation loss: {:.3f}\".format(val_stats[0]))\n    print(\"Validation score: {:.2f}\".format(val_stats[1]))\n    print()\ndef save_test_stats(experiment_dir, test_stats):\n    stats = {'loss': test_stats[0],\n             'dist_accuracy': test_stats[1]}\n    stats_file = os.path.join(experiment_dir, TEST_STATS_FILE)\n    with open(stats_file, 'w') as f:\n        yaml.dump(stats, f, default_flow_style=False)\n        \ndef print_header():\n    '''\n    Print header before train / evaluation run.\n    '''\n    print(\"         Loss  Score\")\n\ndef print_eval_stats(nb_processed, loss, score):\n    '''\n    Log stats during train, evaluation run.\n    '''\n    print(\"  {:5d}: {:.3f}  {:2.2f}\".format(nb_processed, loss, score))\n\n\n# def main(args, force=False):\n\n#   experiment_dir = os.path.join(args.artifact_storage_path, 'metric_learning_emb')\n  \n#   load_path = os.path.join(args.data_storage_path, 'metric_stage_1')\n    \n#   # Maybe return previously trained model\n#   best_net_name = os.path.join(experiment_dir, 'best_model.pkl')\n#   if os.path.isfile(best_net_name) and (not force):\n#     net = load_model(best_net_name)\n#     if not force:\n#       print(\"Best model loaded from previous run. Not forcing training.\")\n#       return net\n\n#   utils.initialize_experiment_if_needed(experiment_dir, evaluate_only=False)\n\n#   train_path = os.path.join(load_path, 'train.pickle')\n#   valid_path = os.path.join(load_path, 'valid.pickle')\n#   test_path  = os.path.join(load_path, 'test.pickle')\n#   stats_path = os.path.join(load_path, 'stats.yml')\n\n#   train_data = Hit_Pair_Dataset(train_path, 10**8)\n#   valid_data = Hit_Pair_Dataset(valid_path, 10**8)\n#   test_data  = Hit_Pair_Dataset(test_path, 10**8)\n\n\n#   train_dataloader = DataLoader(train_dataset,\n#                                 batch_size=args.batch_size,\n#                                 shuffle=True,\n#                                 drop_last=True,\n#                                 pin_memory=True,\n#                                 num_workers=8)\n#   valid_dataloader = DataLoader(valid_dataset,\n#                                 batch_size=args.batch_size,\n#                                 drop_last=True,\n#                                 pin_memory=True,\n#                                 num_workers=8)\n#   test_dataloader  = DataLoader(test_dataset,\n#                                 batch_size=args.batch_size,\n#                                 drop_last=True,\n#                                 pin_memory=True,\n#                                 num_workers=8)\n\n#   net = create_or_restore_model(\n#                                     experiment_dir, \n#                                     train_dataset.get_dim(),\n#                                     args.nb_hidden, \n#                                     args.nb_layer,\n#                                     args.emb_dim,\n#                                     stats_path\n#                                     )\n#   net.to(DEVICE)\n#   if next(net.parameters()).is_cuda:\n#     logging.warning(\"Working on GPU\")\n#     logging.info(\"GPU type:\\n{}\".format(torch.cuda.get_device_name(0)))\n#   else:\n#     logging.warning(\"Working on CPU\")\n    \n#   train(net,\n#         args.lr_start,\n#         args.batch_size,\n#         args.max_nb_epochs,\n#         experiment_dir,\n#         train_dataloader,\n#         valid_dataloader)\n\n#   # Perform evaluation over test set\n#   try:\n#     net = load_best_model(experiment_dir).to(DEVICE)\n#     logging.warning(\"\\nBest model loaded for evaluation on test set.\")\n#   except:\n#     logging.warning(\"\\nCould not load best model for test set. Using current.\")\n#   test_stats = evaluate(net, experiment_dir, args.batch_size, test_dataloader, TEST_NAME)\n#   utils.save_test_stats(experiment_dir, test_stats)\n#   logging.info(\"Test score:  {:3.2f}\".format(test_stats[1]))\n\n#   return net\n\n# input_dim = 4\n# nb_hidden = 128\n# nb_layer = 3\n# emb_dim = 4\n# mean = [0, 0, 0, 0]\n# std = [1, 1, 1, 1]\n# net = MLP(nb_hidden, nb_layer, input_dim, mean, std, emb_dim)\n\n# net.to(DEVICE)\n\n# net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n\n# Now the model should be successfully loaded with the correct state dictionary\n\ninput_dim = 4\nnb_hidden = 64\nnb_layer = 3\nemb_dim = 4\nmean = [0, 0, 0, 0]\nstd = [1, 1, 1, 1]\nnet = MLP(nb_hidden, nb_layer, input_dim, mean, std, emb_dim)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.to(DEVICE)\n\n# Assuming `raw_data1` is your preprocessed raw dataset (a pandas DataFrame)\nraw_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\n\n# Drop the 'eventID' and 'muonID' columns if they exist\n# columns_to_drop = ['eventID', 'muonID']\n# raw_data = raw_data1.drop(columns=columns_to_drop, errors='ignore')\n\n# Convert the DataFrame to a PyTorch tensor\n# raw_tensor = torch.tensor(raw_data.values, dtype=torch.float32).to(DEVICE)  # Move tensor to the appropriate device\nraw_tensor = torch.tensor(raw_data[['pos_x', 'pos_y', 'pos_z', 'time_ns']].values, dtype=torch.float32).to(DEVICE)\n\n# Create a PyTorch Dataset from the tensor\ndataset = TensorDataset(raw_tensor)\n\n# Create a DataLoader for the raw dataset\nbatch_size = 256  # Adjust batch size based on your dataset and computational resources\nraw_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Set the model to evaluation mode\nnet.eval()\n\nraw_tensor = torch.tensor(processed_data[['pos_x', 'pos_y', 'pos_z', 'time_ns']].values, dtype=torch.float32)\nmuon_ids = processed_data['track_no'].values  # Extract muonIDs\nHit_ids = processed_data['HitID'].values  # Extract HitIDs\n\nembeddings = []  # List to store the embeddings\ndistances = []  # List to store the distances\n\nwith torch.no_grad():\n    for batch_data in raw_loader:\n        batch_data = batch_data[0].to(DEVICE)  \n        # Calculate embeddings\n        emb_points = net(batch_data)\n        # Append embeddings to list\n        embeddings.append(emb_points.cpu().numpy())\n        # Calculate pairwise distances\n        pairwise_dist = torch.cdist(emb_points, emb_points, p=2)\n        # Append distances to list\n        distances.append(pairwise_dist.cpu().numpy())\n\nall_embeddings = np.concatenate(embeddings, axis=0)\n\nmax_size = max(arr.shape[0] for arr in distances)\n\npadded_distances = [np.pad(arr, ((0, max_size - arr.shape[0]), (0, max_size - arr.shape[1])), mode='constant') for arr in distances]\n\nall_distances = np.concatenate(padded_distances, axis=1)  # Concatenate along axis 1\n\nnp.save('/kaggle/working/all_embeddings1.npy', all_embeddings)\nnp.save('/kaggle/working/all_distances1.npy', all_distances)\n\nnp.save('/kaggle/working/muon_ids1.npy', muon_ids)\nnp.save('/kaggle/working/Hit_ids1.npy', Hit_ids)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Load the necessary data\nall_embeddings = np.load('/kaggle/working/all_embeddings1.npy')\nmuon_ids = np.load('/kaggle/working/muon_ids1.npy')  # Load track_no values\n\n# Generate unique colors for each track_no\nunique_track_nos = np.unique(muon_ids)\nnum_colors = len(unique_track_nos)\npalette = sns.color_palette(\"husl\", num_colors)  # Use seaborn for distinct colors\ntrack_no_to_color = {track_no: palette[i] for i, track_no in enumerate(unique_track_nos)}\n\n# Assign colors to each point based on track_no\ncolors = [track_no_to_color[track_no] for track_no in muon_ids]\n\n# Scatter plot with unique colors for each track\nplt.figure(figsize=(8, 6))\nplt.scatter(all_embeddings[:, 2], all_embeddings[:, 3], s=10, c=colors, alpha=0.7)\nplt.title(\"Embedded Space with Track Colors\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\n\n\n######graph and edge refinement ########\n\nembedded_points_table = pd.DataFrame({\n    'Index': range(len(all_embeddings)),  # Point indices\n    'track_no': muon_ids,  # muonID for each point\n    'HitID': Hit_ids,  # HitID for each point\n    'Embedded Point': [point.tolist() for point in all_embeddings] \n})\n\nprint(embedded_points_table)\n\nembedded_points_table.to_csv('embedded_points_table', index= False)\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.neighbors import NearestNeighbors\nfrom concurrent.futures import ThreadPoolExecutor\nimport networkx as nx\n\n# ------------------------------\n# Parameters\n# ------------------------------\nepsilon = 5\nbatch_size = 100\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\n\n# Positions and HitIDs\npositions = processed_data[['pos_z', 'time_ns']].values\nhit_indices = processed_data['HitID'].values\n\n# Nearest Neighbors\nneigh = NearestNeighbors(radius=epsilon, algorithm='ball_tree').fit(positions)\n\n# Embeddings\nall_embeddings = np.load('/kaggle/working/all_embeddings1.npy')\nhit_ids = np.load('/kaggle/working/Hit_ids1.npy')\nall_embeddings = torch.tensor(all_embeddings, dtype=torch.float32).cuda()\n\n# Graph directory\nif not os.path.exists('graphs1'):\n    os.makedirs('graphs1')\n\n# ------------------------------\n# Set to Track Processed HitIDs\n# ------------------------------\nprocessed_hit_ids = set()\n\n# ------------------------------\n# Modified Graph Creation Function\n# ------------------------------\ndef process_batch(batch_query_hit_ids, query_indices):\n    \"\"\"Process a batch of query points into graphs.\"\"\"\n    batch_graphs = []\n\n    query_positions = processed_data.loc[processed_data['HitID'].isin(batch_query_hit_ids), ['pos_z', 'time_ns']].values\n    neighbor_indices = neigh.radius_neighbors(query_positions, return_distance=False)\n\n    for query_hit_id, query_idx, neighbors in zip(batch_query_hit_ids, query_indices, neighbor_indices):\n        if query_hit_id in processed_hit_ids:\n            continue\n\n        # Get full position data from DataFrame\n        query_row = processed_data[processed_data['HitID'] == query_hit_id].iloc[0]\n        query_z = query_row['pos_z']\n        query_time = query_row['time_ns']\n        query_point = all_embeddings[query_idx].cpu().numpy()\n\n        # Create graph with full spatial attributes\n        G = nx.Graph()\n        G.add_node(query_hit_id,\n                   features=query_point,\n                   pos_x=query_row['pos_x'],  # Added\n                   pos_y=query_row['pos_y'],  # Added\n                   pos_z=query_z,\n                   time=query_time)\n\n        processed_hit_ids.add(query_hit_id)\n        valid_neighbors = []\n\n        for neighbor_idx in neighbors:\n            if neighbor_idx == query_idx:\n                continue\n\n            neighbor_hit_id = hit_indices[neighbor_idx]\n            if neighbor_hit_id in processed_hit_ids:\n                continue\n\n            # Get neighbor's full position data\n            neighbor_row = processed_data[processed_data['HitID'] == neighbor_hit_id].iloc[0]\n            neighbor_z = neighbor_row['pos_z']\n            neighbor_time = neighbor_row['time_ns']\n            neighbor_point = all_embeddings[neighbor_idx].cpu().numpy()\n\n            # Calculate probabilities with full spatial features\n            z_diff_sqr = (query_z - neighbor_z) ** 2\n            time_diff_sqr = (query_time - neighbor_time) ** 2\n\n            z_prob = 1 / (1 + torch.exp(-10 * torch.tensor(z_diff_sqr, dtype=torch.float32, device='cuda')))\n            time_prob = 1 / (1 + torch.exp(-1 * torch.tensor(time_diff_sqr, dtype=torch.float32, device='cuda')))\n            prob = z_prob * time_prob\n\n            if prob <= 0.6:\n                # Add node with full spatial attributes\n                G.add_node(neighbor_hit_id,\n                           features=neighbor_point,\n                           pos_x=neighbor_row['pos_x'],  # Added\n                           pos_y=neighbor_row['pos_y'],  # Added\n                           pos_z=neighbor_z,\n                           time=neighbor_time)\n                G.add_edge(query_hit_id, neighbor_hit_id)\n                valid_neighbors.append(neighbor_hit_id)\n                processed_hit_ids.add(neighbor_hit_id)\n\n        if len(valid_neighbors) >= 2:\n            batch_graphs.append((query_hit_id, G))\n\n    return batch_graphs\n\n# ------------------------------\n# Main Execution Loop (Unchanged)\n# ------------------------------\nnum_graphs_saved = 0\n\nwith torch.no_grad(), ThreadPoolExecutor() as executor:\n    for batch_start_idx in range(0, len(hit_ids), batch_size):\n        batch_query_hit_ids = hit_ids[batch_start_idx:batch_start_idx + batch_size]\n        batch_query_indices = np.where(np.isin(hit_ids, batch_query_hit_ids))[0]\n\n        batch_graphs = process_batch(batch_query_hit_ids, batch_query_indices)\n        num_graphs_saved += len(batch_graphs)\n\n        for graph in batch_graphs:\n            query_hit_id, G = graph\n            executor.submit(pickle.dump, G, open(f'graphs1/Graph_{query_hit_id}.pkl', 'wb'))\n\nprint(f\"🔥 Number of graphs saved: {num_graphs_saved}\")\n\n\nimport os\nimport pickle\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\n\n# Load the processed data\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\n\n# Create a mapping from HitID to true track_no\nhitid_to_track = dict(zip(processed_data['HitID'], processed_data['track_no']))\n\n# Count hits per true track\ntrue_track_counts = processed_data['track_no'].value_counts().to_dict()\n\n# Directory containing saved graphs\ngraph_dir = 'graphs1'\n\n# Load the reconstructed graphs\ngraphs = []\nfor file in os.listdir(graph_dir):\n    if file.endswith('.pkl'):\n        with open(os.path.join(graph_dir, file), 'rb') as f:\n            G = pickle.load(f)\n            graphs.append(G)\n\nprint(f\"Number of graphs loaded: {len(graphs)}\")\n\n# Extract connected components as reconstructed tracks\nreconstructed_tracks = []\nfor G in graphs:\n    for component in nx.connected_components(G):\n        reconstructed_tracks.append(list(component))  # Each component is a track (list of HitIDs))\n\nprint(f\"Total reconstructed tracks: {len(reconstructed_tracks)}\")\n\n# Create a dictionary to track how true tracks are reconstructed\ntrue_track_reconstruction = {track_no: [] for track_no in true_track_counts}\n\n# Evaluate track purity and efficiency\nvalid_tracks = 0\nvalid_tracks_50 = 0  # For threshold test at 50%\n\nfor track in reconstructed_tracks:\n    true_track_ids = [hitid_to_track[hit] for hit in track if hit in hitid_to_track]\n\n    if len(true_track_ids) == 0:\n        continue\n\n    # Count occurrences of each true track ID in the reconstructed track\n    track_counts = np.bincount(true_track_ids)\n    dominant_track = np.argmax(track_counts)  # The most frequent true track\n    max_hits = track_counts[dominant_track]  # Hits belonging to dominant track\n    total_hits = len(track)  # Total hits in this reconstructed track\n\n    purity = max_hits / total_hits\n\n    # Store how many reconstructed fragments exist for each true track\n    true_track_reconstruction[dominant_track].append(max_hits)\n\n    # Only consider the reconstructed track if purity is ≥ 70%\n    if purity >= 0.7 and max_hits / true_track_counts[dominant_track] >= 0.7:\n        valid_tracks += 1\n    \n    # Also check for a 50% threshold (for debugging)\n    if purity >= 0.5 and max_hits / true_track_counts[dominant_track] >= 0.5:\n        valid_tracks_50 += 1\n\n# Compute efficiency\ntotal_true_tracks = len(true_track_counts)\nefficiency = valid_tracks / total_true_tracks\nefficiency_50 = valid_tracks_50 / total_true_tracks  # Lower threshold check\n\nprint(f\"Corrected Track-Level Efficiency (70% threshold): {efficiency:.3f}\")\nprint(f\"Track-Level Efficiency (50% threshold for debugging): {efficiency_50:.3f}\")\n\n# Debugging: Check how many true tracks were completely missed\nmissing_tracks = [track_no for track_no in true_track_counts if not true_track_reconstruction[track_no]]\nprint(f\"Number of true tracks not reconstructed: {len(missing_tracks)}\")\n\n# Print sample true track fragmentation details\nprint(\"True track fragmentation details:\")\nfor track_no, fragments in list(true_track_reconstruction.items())[:10]:  # Print first 10 for debugging\n    print(f\"True Track {track_no}: {len(fragments)} reconstructed fragments\")\n\n# Debugging: Print a few sample reconstructed tracks\nprint(\"Sample reconstructed tracks:\")\nfor i, track in enumerate(reconstructed_tracks[:15]):  # Print first 5 reconstructed tracks\n    print(f\"Reconstructed Track {i}: {track}\")\nimport os\nimport pickle\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load the processed data\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\n\n# Create a mapping from HitID to true track_no (muonID in this case)\nhitid_to_track = dict(zip(processed_data['HitID'], processed_data['track_no']))\n\n# Extract true track positions\nhit_positions = processed_data.set_index('HitID')[['pos_x', 'pos_y', 'pos_z']]\n\n# Load the reconstructed graphs\ngraph_dir = 'graphs1'\ngraphs = []\nfor file in os.listdir(graph_dir):\n    if file.endswith('.pkl'):\n        with open(os.path.join(graph_dir, file), 'rb') as f:\n            G = pickle.load(f)\n            graphs.append(G)\n\n# Extract connected components as reconstructed tracks\nreconstructed_tracks = []\nfor G in graphs:\n    for component in nx.connected_components(G):\n        reconstructed_tracks.append(list(component))  # Each component is a track (list of HitIDs)\n\n# Select the first 10 reconstructed tracks\nnum_tracks = 5\nselected_reconstructed_tracks = reconstructed_tracks[:num_tracks]\n\n# Find corresponding true tracks\ntrue_tracks = []\nfor track in selected_reconstructed_tracks:\n    # Find the dominant true muonID in this reconstructed track\n    true_hit_ids = [hitid_to_track[hit] for hit in track if hit in hitid_to_track]\n    if true_hit_ids:\n        dominant_muon_id = max(set(true_hit_ids), key=true_hit_ids.count)\n        # Get all HitIDs belonging to this true muonID\n        true_hit_list = list(processed_data[processed_data['track_no'] == dominant_muon_id]['HitID'])\n        true_tracks.append(true_hit_list)\n    else:\n        true_tracks.append([])  # Empty list if no valid true track found\n\n# Function to plot tracks in 3D\ndef plot_tracks(tracks, title, color_map='jet'):\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    colormap = plt.get_cmap(color_map)\n    num_tracks = len(tracks)\n\n    for i, track in enumerate(tracks):\n        if not track:\n            continue\n        valid_hits = [hit for hit in track if hit in hit_positions.index]\n        if not valid_hits:\n            continue  # Skip empty tracks\n        track_hits = hit_positions.loc[valid_hits].values\n        ax.scatter(track_hits[:, 0], track_hits[:, 1], track_hits[:, 2], \n                   marker='o', s=10, color=colormap(i / num_tracks), alpha=0.8)\n\n    ax.set_xlabel('X (mm)')\n    ax.set_ylabel('Y (mm)')\n    ax.set_zlabel('Z (mm)')\n    ax.set_title(title)\n    plt.show()\nplot_tracks(selected_reconstructed_tracks, \"Reconstructed Tracks (First 10)\", color_map='rainbow')\nplot_tracks(true_tracks, \"True Tracks (Corresponding to First 10 Reconstructed)\", color_map='coolwarm')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Remove all files in the output directory\nshutil.rmtree(\"/kaggle/working/\")\nos.mkdir(\"/kaggle/working/\")  # Recreate the directory to avoid errors\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom collections import Counter\n\n# **🔹 Load Processed Data (for Track Labels)**\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\nhit_to_track = dict(zip(processed_data['HitID'], processed_data['track_no']))\n\ngraph_dir = \"graphs1/\"\ngraph_purities = []\n\n# **🔹 Iterate Over Saved Graphs**\nfor filename in os.listdir(graph_dir):\n    if filename.endswith(\".pkl\"):\n        with open(os.path.join(graph_dir, filename), \"rb\") as f:\n            G = pickle.load(f)\n        \n        # **Get Track Labels for Graph Nodes**\n        hit_ids = list(G.nodes())\n        track_labels = [hit_to_track[hit_id] for hit_id in hit_ids if hit_id in hit_to_track]\n\n        if track_labels:\n            # **Compute Track Counts**\n            track_counts = Counter(track_labels)\n            most_common_track_hits = max(track_counts.values())  # Most frequent track count\n            total_hits = len(track_labels)  # Total hits in the graph\n            \n            purity = most_common_track_hits / total_hits\n            graph_purities.append(purity)\n\n# **🔹 Compute Average Purity**\naverage_purity = np.mean(graph_purities)\nprint(f\"✅ Average Graph Purity: {average_purity:.3f}\")\n\n# **🔹 Save Purity Results**\npd.DataFrame({\"Graph Purity\": graph_purities}).to_csv(\"graph_purities.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path1 = '/kaggle/input/new-set/test_1000_low.csv'\ndata = pd.read_csv(file_path1, delimiter = ',')\n\nmodel_path = \"/kaggle/input/new-set/trained_model1.pt\"\nmodel_gnn = '/kaggle/working/edge_classifier.pth'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model without applying it\nmodel_dict = torch.load(model_path, map_location=DEVICE)\n\n# Print the keys to inspect the saved layers\nprint(model_dict.keys())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Directory where graphs are stored\ngraph_dir = 'graphs2'\n\n# List all graph files\ngraph_files = [f for f in os.listdir(graph_dir) if f.endswith('.pkl')]\n\n# Select a few graphs to visualize\nnum_graphs_to_plot = 10  # Change this if needed\nselected_graphs = graph_files[:num_graphs_to_plot]  \n\nfor graph_file in selected_graphs:\n    with open(os.path.join(graph_dir, graph_file), 'rb') as f:\n        G = pickle.load(f)\n\n    plt.figure(figsize=(6, 6))\n    pos = nx.spring_layout(G)  # Layout for better visualization\n    node_colors = [G.nodes[n].get('color', 'gray') for n in G.nodes()]  # Get colors if assigned\n    \n    nx.draw(G, pos, with_labels=True, node_size=300, node_color=node_colors, edge_color='black')\n    plt.title(f\"Graph: {graph_file}\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Remove all files in the output directory\nshutil.rmtree(\"/kaggle/working/\")\nos.mkdir(\"/kaggle/working/\")  # Recreate the directory to avoid errors\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndata_filepath = \"/kaggle/input/new-set/pairwise_data_normalized.csv\"\ndataset = pd.read_csv(data_filepath)\nprint(dataset.columns)  # Print actual column names\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import trackml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install trackml\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --user git+https://github.com/LAL/trackml-library","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Remove all files in the output directory\nshutil.rmtree(\"/kaggle/working/\")\nos.mkdir(\"/kaggle/working/\")  # Recreate the directory to avoid errors\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = '/kaggle/input/new-set/recooutpileup_timemod_uniform_1_100recohitfile.tsv'\ndata = pd.read_csv(file_path, delimiter = '\\t')\n\nmodel_path = \"/kaggle/input/new-set/trained_model1.pt\"\n\nmodel_gnn = '/kaggle/working/track_gnn.pth'\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.neighbors import NearestNeighbors\nfrom concurrent.futures import ThreadPoolExecutor\nimport networkx as nx\n\n\n########################################\n#    Query ball based algorithm      #\n#######################################\n\nimport time as ti\nstart_time = ti.time()\nfrom scipy.sparse import csr_matrix\n\n###   embedded space creation   ####\nchunk_size = 1000  \ndata_chunks = pd.read_csv(file_path, chunksize=chunk_size, delimiter = '\\t')#, delimiter = '\\t'\n# Initialize an empty list to store chunks after processing\nprocessed_chunks = []\n# Process each chunk and assign unique HitID\nfor chunk in data_chunks:    \n     chunk = chunk[chunk['muonID'] != -999]\n\n     if not chunk.empty:\n        # Generate unique HitID based on index\n        chunk['HitID'] = chunk.index + 1\n\n        # Append processed chunk to the list\n        processed_chunks.append(chunk)\ndata = pd.concat(processed_chunks, ignore_index=True)\ndata.to_csv(\"/kaggle/working/processed_data_with_hitid.csv\", index=False)\nprint(data.head())\n####  All the classes and definitions  ####\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 nb_hidden,\n                 nb_layer,\n                 input_dim,\n                 mean,\n                 std,\n                 emb_dim=2):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, nb_hidden)]\n        ln = [nn.Linear(nb_hidden, nb_hidden) for _ in range(nb_layer-1)]\n        layers.extend(ln)\n        self.layers = nn.ModuleList(layers)\n        self.emb_layer = nn.Linear(nb_hidden, emb_dim)\n        self.act = nn.ReLU()\n        self.mean = torch.FloatTensor(mean).to(torch.float)\n        self.std = torch.FloatTensor(std).to(torch.float)\n\n    def forward(self, hits):\n        hits = self.normalize(hits)\n        for l in self.layers:\n            hits = l(hits)\n            hits = self.act(hits)\n            # hits = self.dropout(hits)\n        hits = self.emb_layer(hits)\n        return hits\n\n#     def normalize(self, hits):\n\n#         return hits\n    def normalize(self, hits):\n        min_vals = torch.min(hits, dim=1, keepdim=True)[0]\n        max_vals = torch.max(hits, dim=1, keepdim=True)[0]\n        hits = (hits - min_vals) / (max_vals - min_vals + 1e-10)  # Avoid division by zero\n        return hits\n\n#score calculation: correctly predicted labels/total labels\n\ndef score_dist_accuracy(pred, true):\n    pred_classes = torch.ones_like(pred)  # Initialize all predictions as 1\n    pred_classes[pred >= 0.1] = -1 \n    correct = pred_classes == true\n    nb_correct = correct.sum().item()\n    nb_total = true.size(0)\n    score = nb_correct / nb_total\n    return score\n\n####################\n#     PRINTING     #\n####################\ndef print_header():\n  '''\n  Print header before train / evaluation run.\n  '''\n  logging.info(\"         Loss  Score\")\n\ndef print_eval_stats(nb_processed, loss, score):\n  '''\n  Log stats during train, evaluation run.\n  '''\n  logging.info(\"  {:5d}: {:.3f}  {:2.2f}\".format(nb_processed, loss, score))\n\ninput_dim = 4\nnb_hidden = 64\nnb_layer = 3\nemb_dim = 4\nmean = [0, 0, 0, 0]\nstd = [1, 1, 1, 1]\nnet = MLP(nb_hidden, nb_layer, input_dim, mean, std, emb_dim)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.to(DEVICE)\n\n# Assuming `raw_data1` is your preprocessed raw dataset (a pandas DataFrame)\nraw_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n\n# Move tensor to the appropriate device\nraw_tensor = torch.tensor(raw_data[['pos_x', 'pos_y', 'pos_z', 'time']].values, dtype=torch.float32).to(DEVICE)\n# Create a PyTorch Dataset from the tensor\ndataset = TensorDataset(raw_tensor)\n# Create a DataLoader for the raw dataset\nbatch_size = 256  # Adjust batch size based on your dataset and computational resources\nraw_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n# Set the model to evaluation mode\nnet.eval()\n\nraw_tensor = torch.tensor(processed_data[['pos_x', 'pos_y', 'pos_z', 'time']].values, dtype=torch.float32)\nmuon_ids = processed_data['muonID'].values  # Extract muonIDs\nHit_ids = processed_data['HitID'].values  # Extract HitIDs\n\nembeddings = []  # List to store the embeddings\ndistances = []  # List to store the distances\n\nwith torch.no_grad():\n    for batch_data in raw_loader:\n        batch_data = batch_data[0].to(DEVICE)  \n        # Calculate embeddings\n        emb_points = net(batch_data)\n        # Append embeddings to list\n        embeddings.append(emb_points.cpu().numpy())\n        # Calculate pairwise distances\n        pairwise_dist = torch.cdist(emb_points, emb_points, p=2)\n        # Append distances to list\n        distances.append(pairwise_dist.cpu().numpy())\n\nall_embeddings = np.concatenate(embeddings, axis=0)\nmax_size = max(arr.shape[0] for arr in distances)\npadded_distances = [np.pad(arr, ((0, max_size - arr.shape[0]), (0, max_size - arr.shape[1])), mode='constant') for arr in distances]\nall_distances = np.concatenate(padded_distances, axis=1)  # Concatenate along axis 1\nnp.save('/kaggle/working/all_embeddings.npy', all_embeddings)\nnp.save('/kaggle/working/all_distances.npy', all_distances)\nnp.save('/kaggle/working/muon_ids.npy', muon_ids)\nnp.save('/kaggle/working/Hit_ids.npy', Hit_ids)\n# ------------------------------\n# Parameters\n# ------------------------------\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nimport torch\nfrom concurrent.futures import ThreadPoolExecutor\n\n# ------------------------------\n# Configuration\n# ------------------------------\nepsilon = 6\nbatch_size = 100\ngraph_dir = 'graphs1'\n\n# ------------------------------\n# Data Loading\n# ------------------------------\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\npositions = processed_data[['pos_z', 'time']].values\nhit_indices = processed_data['HitID'].values\n\nneigh = NearestNeighbors(radius=epsilon, algorithm='ball_tree').fit(positions)\n\nall_embeddings = np.load('/kaggle/working/all_embeddings.npy')\nhit_ids = np.load('/kaggle/working/Hit_ids.npy')\nall_embeddings = torch.tensor(all_embeddings, dtype=torch.float32).cuda()\n\nif not os.path.exists(graph_dir):\n    os.makedirs(graph_dir)\n\n# ------------------------------\n# Processed HitIDs Tracker\n# ------------------------------\nprocessed_hit_ids = set()\n\n# ------------------------------\n# Modified Graph Creation Function\n# ------------------------------\ndef process_batch(batch_query_hit_ids, query_indices):\n    \"\"\"Process a batch of query points into graphs.\"\"\"\n    batch_graphs = []\n\n    query_positions = processed_data.loc[processed_data['HitID'].isin(batch_query_hit_ids), ['pos_z', 'time']].values\n    neighbor_indices = neigh.radius_neighbors(query_positions, return_distance=False)\n\n    for query_hit_id, query_idx, neighbors in zip(batch_query_hit_ids, query_indices, neighbor_indices):\n        if query_hit_id in processed_hit_ids:\n            continue\n\n        # Get full position data from DataFrame\n        query_row = processed_data[processed_data['HitID'] == query_hit_id].iloc[0]\n        query_z = query_row['pos_z']\n        query_time = query_row['time']\n        query_point = all_embeddings[query_idx].cpu().numpy()\n\n        # Create graph with full spatial attributes\n        G = nx.Graph()\n        G.add_node(query_hit_id,\n                   features=query_point,\n                   pos_x=query_row['pos_x'],  # Added\n                   pos_y=query_row['pos_y'],  # Added\n                   pos_z=query_z,\n                   time=query_time)\n\n        processed_hit_ids.add(query_hit_id)\n        valid_neighbors = []\n\n        for neighbor_idx in neighbors:\n            if neighbor_idx == query_idx:\n                continue\n\n            neighbor_hit_id = hit_indices[neighbor_idx]\n            if neighbor_hit_id in processed_hit_ids:\n                continue\n\n            # Get neighbor's full position data\n            neighbor_row = processed_data[processed_data['HitID'] == neighbor_hit_id].iloc[0]\n            neighbor_z = neighbor_row['pos_z']\n            neighbor_time = neighbor_row['time']\n            neighbor_point = all_embeddings[neighbor_idx].cpu().numpy()\n\n            # Calculate probabilities with full spatial features\n            z_diff_sqr = (query_z - neighbor_z) ** 2\n            time_diff_sqr = (query_time - neighbor_time) ** 2\n\n            z_prob = 1 / (1 + torch.exp(-10 * torch.tensor(z_diff_sqr, dtype=torch.float32, device='cuda')))\n            time_prob = 1 / (1 + torch.exp(-1 * torch.tensor(time_diff_sqr, dtype=torch.float32, device='cuda')))\n            prob = z_prob * time_prob\n\n            if prob <= 0.6:\n                # Add node with full spatial attributes\n                G.add_node(neighbor_hit_id,\n                           features=neighbor_point,\n                           pos_x=neighbor_row['pos_x'],  # Added\n                           pos_y=neighbor_row['pos_y'],  # Added\n                           pos_z=neighbor_z,\n                           time=neighbor_time)\n                G.add_edge(query_hit_id, neighbor_hit_id)\n                valid_neighbors.append(neighbor_hit_id)\n                processed_hit_ids.add(neighbor_hit_id)\n\n        if len(valid_neighbors) >= 2:\n            batch_graphs.append((query_hit_id, G))\n\n    return batch_graphs\n\n\n# ------------------------------\n# Main Execution Loop (Unchanged)\n# ------------------------------\nnum_graphs_saved = 0\n\nwith torch.no_grad(), ThreadPoolExecutor() as executor:\n    for batch_start_idx in range(0, len(hit_ids), batch_size):\n        batch_query_hit_ids = hit_ids[batch_start_idx:batch_start_idx + batch_size]\n        batch_query_indices = np.where(np.isin(hit_ids, batch_query_hit_ids))[0]\n\n        batch_graphs = process_batch(batch_query_hit_ids, batch_query_indices)\n        num_graphs_saved += len(batch_graphs)\n\n        for graph in batch_graphs:\n            query_hit_id, G = graph\n            executor.submit(pickle.dump, G, open(f'graphs1/Graph_{query_hit_id}.pkl', 'wb'))\n\nprint(f\"🔥 Number of graphs saved: {num_graphs_saved}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Load processed data for track_no lookup\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n\n# Directory where graphs are saved\ngraph_dir = 'graphs1'\n\n# List all raw graph files\nraw_graph_files = [f for f in os.listdir(graph_dir) if f.startswith('raw_Graph_') and f.endswith('.pkl')]\n\n# How many graphs to plot\nimport random\n\n# Shuffle the graph files\nrandom.shuffle(raw_graph_files)\n\n# How many graphs to plot\nnum_graphs_to_plot = 5\nselected_graphs = raw_graph_files[:num_graphs_to_plot]\n\n\nfor raw_graph_file in selected_graphs:\n    query_hit_id = int(raw_graph_file.split('_')[-1].split('.')[0])  # Extract HitID from filename\n\n    raw_path = os.path.join(graph_dir, f'raw_Graph_{query_hit_id}.pkl')\n    filt_path = os.path.join(graph_dir, f'filtered_Graph_{query_hit_id}.pkl')\n\n    if not os.path.exists(filt_path):\n        print(f\"❌ Missing filtered graph for HitID: {query_hit_id}\")\n        continue\n\n    # Load graphs\n    with open(raw_path, 'rb') as f:\n        G_raw = pickle.load(f)\n    with open(filt_path, 'rb') as f:\n        G_filt = pickle.load(f)\n\n    # Get track number for the query\n    try:\n        query_track = processed_data.loc[processed_data['HitID'] == query_hit_id, 'muonID'].values[0]\n    except IndexError:\n        print(f\"❌ HitID {query_hit_id} not found in processed data.\")\n        continue\n\n    # Layout: query in center, neighbors in circle\n    neighbors = list(G_raw.neighbors(query_hit_id))\n    num_neighbors = max(len(neighbors), 1)\n    layout = {query_hit_id: (0, 0)}\n    radius = 1.5\n\n    for i, neighbor in enumerate(neighbors):\n        angle = 2 * np.pi * i / num_neighbors\n        x = radius * np.cos(angle)\n        y = radius * np.sin(angle)\n        layout[neighbor] = (x, y)\n\n    # Add extra neighbors from filtered graph\n    for node in G_filt.nodes():\n        if node not in layout:\n            angle = np.random.rand() * 2 * np.pi\n            layout[node] = (radius * np.cos(angle), radius * np.sin(angle))\n\n    # Helper function to assign colors\n    def assign_colors(G, query_hit_id, query_track):\n        colors = []\n        for node in G.nodes():\n            if node == query_hit_id:\n                colors.append('red')\n            else:\n                try:\n                    node_track = processed_data.loc[processed_data['HitID'] == node, 'muonID'].values[0]\n                    colors.append('blue' if node_track == query_track else 'green')\n                except IndexError:\n                    colors.append('gray')\n        return colors\n\n    colors_raw = assign_colors(G_raw, query_hit_id, query_track)\n    colors_filt = assign_colors(G_filt, query_hit_id, query_track)\n\n    # Plot both graphs side-by-side\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    nx.draw(G_raw, layout, ax=axs[0], with_labels=True, node_size=500, node_color=colors_raw, edge_color='black')\n    axs[0].set_title(\"Original Graph\")\n\n    nx.draw(G_filt, layout, ax=axs[1], with_labels=True, node_size=500, node_color=colors_filt, edge_color='black')\n    axs[1].set_title(\"Filtered Graph (After Sigmoid)\")\n\n    plt.suptitle(f\"Query HitID: {query_hit_id} | Track No: {query_track}\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom collections import Counter\n\n# 🔹 Load Processed Data (for track labels)\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\nhit_to_track = dict(zip(processed_data['HitID'], processed_data['muonID']))\n\n# 🔹 Directory with saved graphs\ngraph_dir = \"/kaggle/working/graphs1\"\n\nraw_purities = []\nfiltered_purities = []\n\n# 🔹 Iterate over filtered graphs (and find matching raw)\nfor filename in os.listdir(graph_dir):\n    if filename.startswith(\"filtered_Graph_\") and filename.endswith(\".pkl\"):\n        query_hit_id = filename.split(\"filtered_Graph_\")[1].split(\".pkl\")[0]\n        raw_path = os.path.join(graph_dir, f\"raw_Graph_{query_hit_id}.pkl\")\n        filtered_path = os.path.join(graph_dir, filename)\n\n        if not os.path.exists(raw_path):\n            continue  # Skip if raw graph is missing\n\n        # Load raw and filtered graphs\n        with open(raw_path, \"rb\") as f:\n            raw_G = pickle.load(f)\n        with open(filtered_path, \"rb\") as f:\n            filtered_G = pickle.load(f)\n\n        def compute_purity(G):\n            hit_ids = list(G.nodes())\n            track_labels = [hit_to_track[hit_id] for hit_id in hit_ids if hit_id in hit_to_track]\n            if not track_labels:\n                return None\n            track_counts = Counter(track_labels)\n            most_common_track_hits = max(track_counts.values())\n            return most_common_track_hits / len(track_labels)\n\n        raw_purity = compute_purity(raw_G)\n        filtered_purity = compute_purity(filtered_G)\n\n        if raw_purity is not None and filtered_purity is not None:\n            raw_purities.append(raw_purity)\n            filtered_purities.append(filtered_purity)\n\n# 🔹 Report average purities\nraw_avg = np.mean(raw_purities) * 100\nfiltered_avg = np.mean(filtered_purities) * 100\n\nprint(f\"📊 Average Raw Graph Purity     : {raw_avg:.2f}%\")\nprint(f\"✅ Average Filtered Graph Purity: {filtered_avg:.2f}%\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = '/kaggle/input/new-set/recooutpileup_timemod_uniform_1_100recohitfile.tsv'\ndata = pd.read_csv(file_path, delimiter='\\t')\n\nmodel_path = \"/kaggle/input/new-set/trained_model1.pt\"\nmodel_gnn = '/kaggle/working/track_gnn.pth'\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.neighbors import NearestNeighbors\nfrom concurrent.futures import ThreadPoolExecutor\nimport networkx as nx\nimport time as ti\n\nstart_time = ti.time()\n\n# ------------------------------\n# Data Processing\n# ------------------------------\nchunk_size = 1000  \ndata_chunks = pd.read_csv(file_path, chunksize=chunk_size, delimiter='\\t')\nprocessed_chunks = []\n\nfor chunk in data_chunks:    \n    chunk = chunk[chunk['muonID'] != -999]\n    if not chunk.empty:\n        chunk['HitID'] = chunk.index + 1\n        processed_chunks.append(chunk)\n\ndata = pd.concat(processed_chunks, ignore_index=True)\ndata.to_csv(\"/kaggle/working/processed_data_with_hitid.csv\", index=False)\nprint(\"Processed data sample:\\n\", data.head())\n\n# ------------------------------\n# MLP Model Definition\n# ------------------------------\nclass MLP(nn.Module):\n    def __init__(self, nb_hidden, nb_layer, input_dim, mean, std, emb_dim=4):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, nb_hidden)]\n        layers.extend([nn.Linear(nb_hidden, nb_hidden) for _ in range(nb_layer-1)])\n        self.layers = nn.ModuleList(layers)\n        self.emb_layer = nn.Linear(nb_hidden, emb_dim)\n        self.act = nn.ReLU()\n        self.mean = torch.FloatTensor(mean)\n        self.std = torch.FloatTensor(std)\n\n    def normalize(self, hits):\n        min_vals = torch.min(hits, dim=1, keepdim=True)[0]\n        max_vals = torch.max(hits, dim=1, keepdim=True)[0]\n        return (hits - min_vals) / (max_vals - min_vals + 1e-10)\n\n    def forward(self, hits):\n        hits = self.normalize(hits)\n        for layer in self.layers:\n            hits = layer(hits)\n            hits = self.act(hits)\n        return self.emb_layer(hits)\n\n# ------------------------------\n# Device Configuration\n# ------------------------------\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# ------------------------------\n# Model Initialization\n# ------------------------------\ninput_dim = 4\nnb_hidden = 64\nnb_layer = 3\nemb_dim = 4\nmean = [0, 0, 0, 0]\nstd = [1, 1, 1, 1]\nnet = MLP(nb_hidden, nb_layer, input_dim, mean, std, emb_dim).to(DEVICE)\nnet.load_state_dict(torch.load(model_path))\nnet.eval()\n\n# ------------------------------\n# Data Loading & Embedding Generation\n# ------------------------------\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n\n# Generate embeddings\nwith torch.no_grad():\n    input_tensor = torch.tensor(\n        processed_data[['pos_x', 'pos_y', 'pos_z', 'time']].values,\n        dtype=torch.float32,\n        device=DEVICE\n    )\n    all_embeddings = net(input_tensor).cpu().numpy()\n\n# Save embeddings and IDs\nhit_ids = processed_data['HitID'].values\nnp.save('/kaggle/working/all_embeddings.npy', all_embeddings)\nnp.save('/kaggle/working/Hit_ids.npy', hit_ids)\n\n# ------------------------------\n# Graph Creation Parameters\n# ------------------------------\nepsilon = 1  # Increased search radius\nprob_threshold = 0.5  # More inclusive threshold\nbatch_size = 1000  # Larger batch size for efficiency\nmin_neighbors = 2  # Minimum neighbors required\n\n# ------------------------------\n# Neighbor Search Setup\n# ------------------------------\npositions = processed_data[['pos_z', 'time']].values\nneigh = NearestNeighbors(radius=epsilon, algorithm='ball_tree')\nneigh.fit(positions)\n\n# ------------------------------\n# Graph Creation Function (Fixed)\n# ------------------------------\nprocessed_hit_ids = set()\n\ndef process_batch(batch_query_hit_ids):\n    batch_graphs = []\n    query_indices = np.where(np.isin(hit_ids, batch_query_hit_ids))[0]\n    \n    if len(query_indices) == 0:\n        return batch_graphs\n\n    query_positions = positions[query_indices]\n    neighbor_indices = neigh.radius_neighbors(query_positions, return_distance=False)\n\n    for q_hit_id, q_idx, neighbors in zip(batch_query_hit_ids, query_indices, neighbor_indices):\n        if q_hit_id in processed_hit_ids:\n            continue\n\n        # Create graph even if no neighbors\n        G = nx.Graph()\n        q_data = processed_data[processed_data['HitID'] == q_hit_id].iloc[0]\n        G.add_node(q_hit_id,\n                   features=all_embeddings[q_idx],\n                   pos_z=q_data['pos_z'],\n                   time=q_data['time'])\n\n        valid_neighbors = []\n        for n_idx in neighbors:\n            if n_idx == q_idx:\n                continue\n\n            n_hit_id = hit_ids[n_idx]\n            if n_hit_id in processed_hit_ids:\n                continue\n\n            n_data = processed_data[processed_data['HitID'] == n_hit_id].iloc[0]\n            \n            # Calculate connection probability\n            z_diff = q_data['pos_z'] - n_data['pos_z']\n            time_diff = q_data['time'] - n_data['time']\n            z_prob = 1 / (1 + np.exp(-10 * z_diff**2))\n            time_prob = 1 / (1 + np.exp(-1 * time_diff**2))\n            prob = z_prob * time_prob\n\n            if prob >= prob_threshold:\n                G.add_node(n_hit_id,\n                           features=all_embeddings[n_idx],\n                           pos_z=n_data['pos_z'],\n                           time=n_data['time'])\n                G.add_edge(q_hit_id, n_hit_id)\n                valid_neighbors.append(n_hit_id)\n\n        # Always save the graph regardless of neighbor count\n        batch_graphs.append((q_hit_id, G))\n        processed_hit_ids.add(q_hit_id)\n        processed_hit_ids.update(valid_neighbors)\n\n    return batch_graphs\n\n# ------------------------------\n# Main Processing Loop\n# ------------------------------\nif not os.path.exists('graphs'):\n    os.makedirs('graphs')\n\nall_hit_ids = processed_data['HitID'].unique()\nnum_graphs_saved = 0\n\nwith ThreadPoolExecutor() as executor:\n    for i in range(0, len(all_hit_ids), batch_size):\n        batch_ids = all_hit_ids[i:i + batch_size]\n        batch_graphs = process_batch(batch_ids)\n        num_graphs_saved += len(batch_graphs)\n\n        # Save graphs in parallel\n        futures = []\n        for q_hit_id, G in batch_graphs:\n            future = executor.submit(\n                pickle.dump, \n                G, \n                open(f'graphs/Graph_{q_hit_id}.pkl', 'wb')\n            )\n            futures.append(future)\n        \n        # Wait for completion of this batch\n        for future in futures:\n            future.result()\n\nprint(f\"🔥 Total graphs saved: {num_graphs_saved}\")\n\n# ------------------------------\n# Post-Processing Validation\n# ------------------------------\nall_processed = set(processed_hit_ids)\nmissing_hits = set(all_hit_ids) - all_processed\n\nprint(f\"\\nValidation Results:\")\nprint(f\"Original HitIDs: {len(all_hit_ids)}\")\nprint(f\"Processed HitIDs: {len(all_processed)}\")\nprint(f\"Missing HitIDs: {len(missing_hits)} ({len(missing_hits)/len(all_hit_ids):.2%})\")\n\n\n\n\nprint(f\"\\nTotal execution time: {ti.time() - start_time:.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport os\nimport networkx as nx\n\n# Load original HitIDs\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\noriginal_hit_ids = set(processed_data['HitID'].values)\n\n# Load HitIDs from saved graphs\ngraph_hit_ids = set()\ngraph_dir = 'graphs1'\n\nfor filename in os.listdir(graph_dir):\n    if filename.endswith('.pkl'):\n        with open(os.path.join(graph_dir, filename), 'rb') as f:\n            G = pickle.load(f)  # Load graph\n            graph_hit_ids.update(G.nodes)  # Collect all HitIDs in the graph\n\n# Compare counts\nprint(f\"Original number of HitIDs: {len(original_hit_ids)}\")\nprint(f\"Number of HitIDs after graph creation: {len(graph_hit_ids)}\")\n\n# Check if any HitIDs are missing\nmissing_hit_ids = original_hit_ids - graph_hit_ids\nextra_hit_ids = graph_hit_ids - original_hit_ids\n\nif missing_hit_ids:\n    print(f\"⚠️ {len(missing_hit_ids)} HitIDs missing after graph creation!\")\n    print(f\"Examples of missing HitIDs: {list(missing_hit_ids)[:10]}\")\nelse:\n    print(\"✅ No HitIDs were lost during graph creation.\")\n\nif extra_hit_ids:\n    print(f\"⚠️ {len(extra_hit_ids)} unexpected HitIDs found in the graphs!\")\n    print(f\"Examples of extra HitIDs: {list(extra_hit_ids)[:10]}\")\nelse:\n    print(\"✅ No unexpected HitIDs were added.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom collections import Counter\n\n# **🔹 Load Processed Data (for Track Labels)**\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\nhit_to_track = dict(zip(processed_data['HitID'], processed_data['track_no']))\n\ngraph_dir = \"graphs1/\"\ngraph_purities = []\n\n# **🔹 Iterate Over Saved Graphs**\nfor filename in os.listdir(graph_dir):\n    if filename.endswith(\".pkl\"):\n        with open(os.path.join(graph_dir, filename), \"rb\") as f:\n            G = pickle.load(f)\n        \n        # **Get Track Labels for Graph Nodes**\n        hit_ids = list(G.nodes())\n        track_labels = [hit_to_track[hit_id] for hit_id in hit_ids if hit_id in hit_to_track]\n\n        if track_labels:\n            # **Compute Track Counts**\n            track_counts = Counter(track_labels)\n            most_common_track_hits = max(track_counts.values())  # Most frequent track count\n            total_hits = len(track_labels)  # Total hits in the graph\n            \n            purity = most_common_track_hits / total_hits\n            graph_purities.append(purity)\n\n# **🔹 Compute Average Purity**\naverage_purity = (np.mean(graph_purities))*100\nprint(f\"✅ Average Graph Purity: {average_purity:.3f}%\")\n\n# **🔹 Save Purity Results**\npd.DataFrame({\"Graph Purity\": graph_purities}).to_csv(\"graph_purities.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicate graphs\ngraph_ids = [file.split('_')[1].split('.')[0] for file in os.listdir(graph_dir)]\nprint(f\"Number of unique graphs: {len(set(graph_ids))}\")\nprint(f\"Total graphs saved: {len(graph_ids)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nimport os\n\n# Load one sample graph\ngraph_path = '/kaggle/working/graphs'  # Replace with your actual graph directory\nsample_graph_file = next((f for f in os.listdir(graph_path) if f.endswith('.pkl')), None)\n\nif sample_graph_file:\n    sample_graph = pickle.load(open(os.path.join(graph_path, sample_graph_file), 'rb'))\n\n    # Inspect the graph properties\n    print(f\"Graph: {sample_graph_file}\")\n    print(\"Nodes and their attributes:\")\n\n    # Print the first few nodes and their attributes\n    for node, data in list(sample_graph.nodes(data=True))[:5]:\n        print(f\"Node {node}: {data}\")\n\n    # Check edges\n    print(\"\\nSample Edges:\")\n    print(list(sample_graph.edges(data=True))[:5])\nelse:\n    print(\"No graph files found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data, Dataset, DataLoader, Batch\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport itertools\nfrom sklearn.utils import shuffle \n\n# ------------------------------\n# Enhanced Configuration\n# ------------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprocessed_data_path = '/kaggle/working/processed_data_with_hitid.csv'\ngraph_dir = 'graphs'\nmodel_dir = 'saved_models'\ncheckpoint_dir = os.path.join(model_dir, 'checkpoints')\n\nos.makedirs(model_dir, exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Enhanced training parameters\ntest_size = 0.2\nrandom_state = 42\nepochs = 30\ncheckpoint_freq = 5\nbatch_size = 128\nmax_pairs_per_track = 20\n\n# Regularization parameters\ndropout_rate = 0.5\nweight_decay = 1e-4\nmax_grad_norm = 1.0\n\n# ------------------------------\n# Data Loading & Preparation\n# ------------------------------\ndef load_graph_metadata():\n    print(\"⏳ Loading graph metadata...\")\n    processed_data = pd.read_csv(processed_data_path)\n    hitid_to_track = dict(zip(processed_data['HitID'], processed_data['muonID']))\n    \n    metadata = []\n    graph_files = [f for f in os.listdir(graph_dir) if f.endswith('.pkl')]\n    \n    for gf in tqdm(graph_files, desc=\"Processing graphs\"):\n        hit_id = int(gf.split('_')[1].split('.')[0])\n        metadata.append({\n            'file_path': os.path.join(graph_dir, gf),\n            'hit_id': hit_id,\n            'track_id': hitid_to_track[hit_id]\n        })\n    \n    return pd.DataFrame(metadata)\n\n# ------------------------------\n# Enhanced Pair Generation\n# ------------------------------\ndef generate_pairs(metadata):\n    print(\"\\n🎯 Generating balanced pairs...\")\n    positive_pairs = []\n    negative_pairs = []\n    negative_types = []  # Track negative pair types\n    \n    track_groups = metadata.groupby('track_id')\n    track_dict = {track: group['file_path'].tolist() \n                 for track, group in track_groups}\n    \n    # Generate positive pairs\n    for track, graphs in tqdm(track_dict.items(), desc=\"Positive pairs\"):\n        if len(graphs) >= 2:\n            possible_pairs = list(itertools.combinations(graphs, 2))\n            n_pairs = min(len(possible_pairs), max_pairs_per_track)\n            positive_pairs.extend(random.sample(possible_pairs, n_pairs))\n    \n    num_positive = len(positive_pairs)\n    print(f\"✅ Generated {num_positive} positive pairs\")\n    \n    # Track features for hard negative mining\n    track_features = {}\n    for track, paths in track_dict.items():\n        with open(paths[0], 'rb') as f:\n            G = pickle.load(f)\n        track_features[track] = list(G.nodes(data=True))[0][1]['features']\n    \n    # Generate negatives with type tracking\n    tracks = list(track_dict.keys())\n    existing_pairs = set()\n    hard_neg_ratio = 0.9\n    \n    with tqdm(total=num_positive, desc=\"Generating negatives\") as pbar:\n        while len(negative_pairs) < num_positive:\n            if random.random() < hard_neg_ratio:\n                # Hard negative generation\n                track1 = random.choice(tracks)\n                similarities = [\n                    (np.dot(track_features[track1], track_features[t2]), t2)\n                    for t2 in random.sample(tracks, min(50, len(tracks)))\n                    if t2 != track1\n                ]\n                if similarities:\n                    track2 = max(similarities)[1]\n                    neg_type = 'hard'\n                else:\n                    continue\n            else:\n                # Random negative\n                track1, track2 = random.sample(tracks, 2)\n                neg_type = 'random'\n            \n            try:\n                g1 = random.choice(track_dict[track1])\n                g2 = random.choice(track_dict[track2])\n            except (KeyError, IndexError):\n                continue\n            \n            pair = frozenset({g1, g2})\n            if pair not in existing_pairs:\n                negative_pairs.append((g1, g2))\n                negative_types.append(neg_type)\n                existing_pairs.add(pair)\n                pbar.update(1)\n    \n    print(f\"✅ Generated {len(negative_pairs)} negative pairs \"\n          f\"({sum(t == 'hard' for t in negative_types)} hard, \"\n          f\"{sum(t == 'random' for t in negative_types)} random)\")\n    \n    # Create stratified labels\n    strat_labels = ['positive'] * num_positive + negative_types\n    all_pairs = positive_pairs + negative_pairs\n    all_labels = [1] * num_positive + [0] * num_positive\n    \n    # Shuffle maintaining stratification\n    all_pairs, all_labels, strat_labels = shuffle(\n        all_pairs, all_labels, strat_labels, \n        random_state=42\n    )\n    \n    return all_pairs, all_labels, strat_labels\n# ------------------------------\n# Enhanced Dataset Class with Validation\n# ------------------------------\nclass GraphPairDataset(Dataset):\n    def __init__(self, pairs, labels):\n        super().__init__()\n        self.pairs = pairs\n        self.labels = labels\n        self.cache = {}\n        \n    def validate_graph(self, data):\n        \"\"\"Edge index validation\"\"\"\n        if data.edge_index.max() >= data.num_nodes:\n            raise ValueError(f\"Edge index exceeds node count (max {data.num_nodes-1})\")\n        if data.edge_index.min() < 0:\n            raise ValueError(\"Negative edge indices detected\")\n        # Removed duplicate edge check to handle undirected graphs\n\n    def load_graph(self, path):\n        if path not in self.cache:\n            with open(path, 'rb') as f:\n                G = pickle.load(f)\n            \n            nodes = list(G.nodes())\n            node_id_to_idx = {n: i for i, n in enumerate(nodes)}\n            \n            edge_list = []\n            seen_edges = set()  # Prevent duplicate edges\n            for u, v in G.edges():\n                if u not in node_id_to_idx or v not in node_id_to_idx:\n                    raise ValueError(f\"Invalid edge ({u}->{v}) in {path}\")\n                src = node_id_to_idx[u]\n                dst = node_id_to_idx[v]\n                \n                # Check for reverse duplicates\n                if (src, dst) in seen_edges or (dst, src) in seen_edges:\n                    continue\n                seen_edges.add((src, dst))\n                \n                edge_list.append([src, dst])\n            \n            data = Data(\n                x=torch.tensor([G.nodes[n]['features'] for n in nodes], dtype=torch.float32),\n                edge_index=torch.tensor(edge_list).t().contiguous(),\n                num_nodes=len(nodes)\n            )\n            \n            self.validate_graph(data)\n            self.cache[path] = data\n            \n        return self.cache[path]\n    \n    def len(self):\n        return len(self.pairs)\n    \n    def get(self, idx):\n        g1_path, g2_path = self.pairs[idx]\n        return (\n            self.load_graph(g1_path), \n            self.load_graph(g2_path), \n            torch.tensor(self.labels[idx], dtype=torch.float32)\n        )\n# ------------------------------\n# Enhanced GNN with Edge Processing\n# ------------------------------\nclass EnhancedEdgeNetwork(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(2*input_dim, 128),\n            nn.Tanh(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        self.edge_mlp = nn.Sequential(\n            nn.Linear(2*input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n    \n    def forward(self, src, dest):\n        attn_weights = self.attention(torch.cat([src, dest], dim=-1))\n        return attn_weights * self.edge_mlp(torch.cat([src, dest], dim=-1))\n\nclass TrackGNN(torch.nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=512, embed_dim=256, n_iter=3):\n        super().__init__()\n        self.n_iter = n_iter\n        \n        # GCN Layers with increased capacity\n        self.convs = nn.ModuleList([\n            GCNConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        # Enhanced edge networks\n        self.edge_nets = nn.ModuleList([\n            EnhancedEdgeNetwork(input_dim if i == 0 else hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        # Residual connections\n        self.res_lin = nn.ModuleList([\n            nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        # Batch Norms for stability\n        self.bns = nn.ModuleList([\n            nn.BatchNorm1d(hidden_dim) for _ in range(n_iter)\n        ])\n        \n        # Enhanced embedding projection\n        self.lin = nn.Sequential(\n            nn.Linear(hidden_dim, embed_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Scaled-up classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(2*embed_dim, 512),  # Increased to match new embed_dim\n            nn.ReLU(),\n            nn.LayerNorm(512),\n            nn.Dropout(0.2),\n            nn.Linear(512, 256),          # Scaled intermediate layer\n            nn.ReLU(),\n            nn.LayerNorm(256),\n            nn.Linear(256, 1),            # Final projection\n            nn.Sigmoid()\n        )\n\n    def forward(self, data1, data2):\n        # [Unchanged forward implementation]\n        def process(data):\n            x, edge_index = data.x, data.edge_index\n            edge_row, edge_col = edge_index\n            \n            for i in range(self.n_iter):\n                edge_weights = torch.sigmoid(\n                    self.edge_nets[i](x[edge_row], x[edge_col])\n                ).squeeze()\n                \n                x_new = F.relu(self.bns[i](self.convs[i](x, edge_index, edge_weights)))\n                x = x_new + self.res_lin[i](x)\n                \n                if self.training:\n                    x += torch.randn_like(x) * 0.01\n                    \n            return global_mean_pool(self.lin(x), data.batch)\n        \n        emb1 = process(data1)\n        emb2 = process(data2)\n        return self.classifier(torch.cat([emb1, emb2], dim=-1)).squeeze()\n\n# Rest of the Code Remains Unchanged\n# ------------------------------\ndef collate_fn(batch):\n    data1_list, data2_list, label_list = [], [], []\n    for data1, data2, label in batch:\n        data1_list.append(data1)\n        data2_list.append(data2)\n        label_list.append(label)\n    return (\n        Batch.from_data_list(data1_list),\n        Batch.from_data_list(data2_list),\n        torch.stack(label_list)\n    )\n\ndef save_model(model, history, suffix=None):\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    model_name = f'track_gnn_{timestamp}' if not suffix else f'track_gnn_{suffix}'\n    torch.save({\n        'model_state': model.state_dict(),\n        'history': history,\n        'config': {\n            'input_dim': model.convs[0].in_channels,\n            'hidden_dim': model.convs[0].out_channels,\n            'embed_dim': model.lin[0].out_features  # Access first layer of Sequential\n        }\n    }, os.path.join(model_dir, f'{model_name}.pt'))\n    print(f\"💾 Model saved as {model_name}.pt\")\n\n\ndef load_model(model_path):\n    checkpoint = torch.load(model_path, map_location=device)\n    model = TrackGNN(\n        input_dim=checkpoint['config']['input_dim'],\n        hidden_dim=checkpoint['config']['hidden_dim'],\n        embed_dim=checkpoint['config']['embed_dim']\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state'])\n    return model, checkpoint['history']\n\ndef plot_history(history):\n    plt.figure(figsize=(12, 5))\n    \n    # Loss Curves\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['test_loss'], label='Test Loss')\n    plt.title('Loss Curves')\n    plt.xlabel('Epochs')         # Added x-axis label\n    plt.ylabel('Loss')           # Added y-axis label\n    plt.legend()\n\n    # Accuracy Curves\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_acc'], label='Train Accuracy')\n    plt.plot(history['test_acc'], label='Test Accuracy')\n    plt.title('Accuracy Curves')\n    plt.xlabel('Epochs')         # Added x-axis label\n    plt.ylabel('Accuracy')       # Added y-axis label\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\ndef train_model():\n    # Load and prepare data\n    metadata_df = load_graph_metadata()\n    pairs, labels, strat_labels = generate_pairs(metadata_df)\n    \n    # Enhanced stratified split\n    X_train, X_test, y_train, y_test = train_test_split(\n        pairs, labels,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=strat_labels  # Stratify by pair type\n    )\n    \n    # Verify stratification\n    # Get indices for all pairs\n    all_indices = np.arange(len(pairs))\n    \n    # Get boolean mask for training samples\n    in_train = np.zeros(len(pairs), dtype=bool)\n    train_indices, _ = train_test_split(\n        all_indices,\n        test_size=test_size,\n        stratify=strat_labels,\n        random_state=random_state\n    )\n    in_train[train_indices] = True\n    \n    # Get actual stratification labels\n    train_strat = [s for s, keep in zip(strat_labels, in_train) if keep]\n    test_strat = [s for s, keep in zip(strat_labels, in_train) if not keep]\n    \n    print(\"\\nStratification Report:\")\n    print(f\"Train: {len(X_train)} pairs \"\n          f\"({train_strat.count('positive')/len(X_train):.1%} pos, \"\n          f\"{train_strat.count('hard')/len(X_train):.1%} hard neg, \"\n          f\"{train_strat.count('random')/len(X_train):.1%} rand neg)\")\n    print(f\"Test:  {len(X_test)} pairs \"\n          f\"({test_strat.count('positive')/len(X_test):.1%} pos, \"\n          f\"{test_strat.count('hard')/len(X_test):.1%} hard neg, \"\n          f\"{test_strat.count('random')/len(X_test):.1%} rand neg)\")\n\n    # Initialize model components\n    model = TrackGNN().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n    criterion = torch.nn.BCELoss()\n\n    # Create datasets and loaders\n    train_dataset = GraphPairDataset(X_train, y_train)\n    test_dataset = GraphPairDataset(X_test, y_test)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\n    # Training history tracking\n    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n\n    # Training loop\n    for epoch in range(epochs):\n        print(f\"\\n🎯 Epoch {epoch+1}/{epochs}\")\n        model.train()\n        train_loss, train_correct = 0, 0\n        \n        with tqdm(train_loader, unit=\"batch\", desc=\"Training\") as tepoch:\n            for data1, data2, labels in tepoch:\n                data1, data2 = data1.to(device), data2.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(data1, data2)\n                \n                assert outputs.shape == labels.shape, \\\n                    f\"Shape mismatch: outputs {outputs.shape}, labels {labels.shape}\"\n                \n                loss = criterion(outputs, labels)\n                loss.backward()\n                \n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n                optimizer.step()\n                \n                train_loss += loss.item()\n                train_correct += ((outputs > 0.5).float() == labels).sum().item()\n                tepoch.set_postfix(loss=loss.item())\n\n        model.eval()\n        test_loss, test_correct = 0, 0\n        with torch.no_grad(), tqdm(test_loader, unit=\"batch\", desc=\"Testing\") as tepoch:\n            for data1, data2, labels in tepoch:\n                data1, data2 = data1.to(device), data2.to(device)\n                labels = labels.to(device)\n                \n                outputs = model(data1, data2)\n                test_loss += criterion(outputs, labels).item()\n                test_correct += ((outputs > 0.5).float() == labels).sum().item()\n                tepoch.set_postfix(loss=criterion(outputs, labels).item())\n\n        scheduler.step(test_loss/len(test_loader))\n\n        train_loss_avg = train_loss/len(train_loader)\n        test_loss_avg = test_loss/len(test_loader)\n        train_acc = train_correct/len(X_train)\n        test_acc = test_correct/len(X_test)\n        \n        history['train_loss'].append(train_loss_avg)\n        history['test_loss'].append(test_loss_avg)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n\n        if (epoch+1) % checkpoint_freq == 0:\n            checkpoint_name = f'checkpoint_epoch_{epoch+1}.pt'\n            torch.save({\n                'model_state': model.state_dict(),\n                'history': history,\n                'epoch': epoch+1\n            }, os.path.join(checkpoint_dir, checkpoint_name))\n            print(f\"💾 Saved checkpoint at epoch {epoch+1}\")\n\n        print(f\"Train Loss: {train_loss_avg:.4f} | Test Loss: {test_loss_avg:.4f}\")\n        print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n    save_model(model, history)\n    plot_history(history)\n    \n    return model, history\nif __name__ == \"__main__\":\n    trained_model, training_history = train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.nn import global_mean_pool\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.neighbors import NearestNeighbors\nimport torch.nn.functional as F\n\n\n# 1. Define Model Architecture (Must match training exactly)\nclass EnhancedEdgeNetwork(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 128),\n            torch.nn.Tanh(),\n            torch.nn.Linear(128, 1),\n            torch.nn.Sigmoid()\n        )\n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 1)\n        )\n    \n    def forward(self, src, dest):\n        attn_weights = self.attention(torch.cat([src, dest], dim=-1))\n        return attn_weights * self.edge_mlp(torch.cat([src, dest], dim=-1))\n\nclass TrackGNN(torch.nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=512, embed_dim=256, n_iter=3):\n        super().__init__()\n        self.n_iter = n_iter\n        \n        # Original encoder components\n        self.convs = torch.nn.ModuleList([\n            GCNConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.edge_nets = torch.nn.ModuleList([\n            EnhancedEdgeNetwork(input_dim if i == 0 else hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.res_lin = torch.nn.ModuleList([\n            torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.bns = torch.nn.ModuleList([\n            torch.nn.BatchNorm1d(hidden_dim) for _ in range(n_iter)\n        ])\n        \n        self.lin = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, embed_dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.3)\n        )\n\n        # Add missing classifier with exact architecture from training\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(2*embed_dim, 512),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(512),\n            torch.nn.Dropout(0.2),\n            torch.nn.Linear(512, 256),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(256),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, data):\n        # Keep original forward logic\n        x, edge_index = data.x, data.edge_index\n        edge_row, edge_col = edge_index\n        \n        for i in range(self.n_iter):\n            edge_weights = torch.sigmoid(\n                self.edge_nets[i](x[edge_row], x[edge_col])\n            ).squeeze()\n            \n            x_new = F.relu(self.bns[i](self.convs[i](x, edge_index, edge_weights)))\n            x = x_new + self.res_lin[i](x)\n            \n        return global_mean_pool(self.lin(x), data.batch)\n\n\n# 2. Dataset Class for Inference\nfrom torch_geometric.data import Dataset\n\nclass GraphDataset(Dataset):\n    def __init__(self, graph_dir):\n        super().__init__()\n        self.graph_dir = graph_dir\n        self.file_list = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pkl')])\n        self.hit_ids = [self._extract_hit_id(f) for f in self.file_list]  # Store hit IDs\n        self.cache = {}\n\n    def _extract_hit_id(self, filename):\n        \"\"\"Extract HitID from filename (e.g., Graph_12345.pkl -> 12345)\"\"\"\n        return int(filename.split('_')[1].split('.')[0])\n\n    def load_graph(self, path):\n        if path not in self.cache:\n            with open(path, 'rb') as f:\n                G = pickle.load(f)\n            \n            nodes = list(G.nodes())\n            node_id_to_idx = {n: i for i, n in enumerate(nodes)}\n            \n            edge_list = []\n            seen_edges = set()\n            for u, v in G.edges():\n                src = node_id_to_idx[u]\n                dst = node_id_to_idx[v]\n                if (src, dst) not in seen_edges and (dst, src) not in seen_edges:\n                    edge_list.append([src, dst])\n                    seen_edges.add((src, dst))\n            \n            self.cache[path] = Data(\n                x=torch.tensor([G.nodes[n]['features'] for n in nodes], dtype=torch.float32),\n                edge_index=torch.tensor(edge_list).t().contiguous(),\n                num_nodes=len(nodes)\n            )\n        return self.cache[path]\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        return self.load_graph(os.path.join(self.graph_dir, self.file_list[idx]))\n\n    def get_hit_ids(self):\n        \"\"\"Get all HitIDs in dataset order\"\"\"\n        return self.hit_ids\n\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch_geometric.data import Data, DataLoader\n\n\ndef merge_tracks(model_path, test_graph_dir, output_csv):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load original data with validation\n    processed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid1.csv')\n    if 'track_no' not in processed_data.columns:\n        raise ValueError(\"Processed data missing 'muonID' column\")\n    original_tracks = processed_data['track_no'].nunique()\n    \n    # Load model with safety checks\n    checkpoint = torch.load(model_path, map_location=device)\n    model = TrackGNN(\n        input_dim=checkpoint['config'].get('input_dim', 4),\n        hidden_dim=checkpoint['config'].get('hidden_dim', 512),\n        embed_dim=checkpoint['config'].get('embed_dim', 256)\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state'], strict=False)\n    model.eval()\n\n    # Batch processing setup\n    dataset = GraphDataset(test_graph_dir)\n    hit_ids = dataset.hit_ids\n    \n    loader = DataLoader(\n        dataset,\n        batch_size=64,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    # Process graphs in batches\n    embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Processing graphs\"):\n            batch = batch.to(device)\n            embeddings.append(model(batch).cpu().numpy())\n    \n    X = np.concatenate(embeddings)\n    X = StandardScaler().fit_transform(X)\n\n    # Density-based clustering with updated parameters\n    clusterer = DBSCAN(\n        eps=0.5,        # Increased epsilon\n        min_samples=2,   # Reduced minimum points\n        metric='euclidean',\n        n_jobs=-1\n    ).fit(X)\n    \n    # Create results without size constraints\n    results = pd.DataFrame({\n        'HitID': hit_ids,\n        'TrackID': clusterer.labels_,\n    }).merge(\n        processed_data[['HitID', 'track_no']],\n        on='HitID',\n        how='left'\n    )\n\n    # Cluster validation without size limits\n    cluster_stats = results[results['TrackID'] != -1].groupby('TrackID').agg(\n        size=('HitID', 'count'),\n        purity=('track_no', lambda x: x.value_counts(normalize=True).iloc[0])\n    ).reset_index()\n\n    # Remove all size constraints\n    min_size = 1  # Absolute minimum\n    min_purity = 0.65  # Relaxed purity threshold\n\n    # Final validation (only purity check)\n    valid_tracks = cluster_stats[\n        (cluster_stats['purity'] >= min_purity)\n    ]\n\n    # Efficiency calculation\n    if valid_tracks.empty:\n        efficiency = 0.0\n    else:\n        efficiency = valid_tracks.merge(\n            results[['TrackID', 'track_no']].drop_duplicates(),\n            on='TrackID',\n            how='left'\n        )['track_no'].nunique() / original_tracks\n\n    # Diagnostic reporting\n    print(\"\\n=== Clustering Diagnostics ===\")\n    print(f\"Total clusters: {len(cluster_stats)}\")\n    print(f\"Noise points: {(results['TrackID'] == -1).sum()} ({(results['TrackID'] == -1).mean():.1%})\")\n    print(f\"Cluster size range: {cluster_stats['size'].min()}-{cluster_stats['size'].max()}\")\n    print(f\"Validation thresholds: purity ≥ {min_purity:.2f}\")\n    \n    print(\"\\n=== Final Results ===\")\n    print(f\"Original muon tracks: {original_tracks}\")\n    print(f\"Valid reconstructed tracks: {len(valid_tracks)}\")\n    print(f\"Tracking efficiency: {efficiency:.2%}\")\n\n    results.to_csv(output_csv, index=False)\n    return results, valid_tracks, efficiency, X  # Add X to the return\n\n\n\n# Call the function and get results + embeddings\nresults, stats, efficiency, X = merge_tracks(\n    model_path=\"/kaggle/input/new-set/track_gnn_best.pt\",\n    test_graph_dir=\"/kaggle/working/graphs1\",\n    output_csv=\"merged_tracks.csv\"\n)\n\n# Now do the t-SNE visualization\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2, perplexity=40, random_state=42)\nX_2d = tsne.fit_transform(X)\n\n# Merge X_2d with labels\nresults['x'] = X_2d[:, 0]\nresults['y'] = X_2d[:, 1]\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(results['x'], results['y'], c=results['TrackID'], cmap='tab20', s=10)\nplt.title(\"t-SNE visualization of merged track clusters\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.colorbar(scatter, label=\"TrackID\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Optional: Reduce dimensions using t-SNE for visualization\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nX_2d = tsne.fit_transform(X)\n\n# Plot\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=clusterer.labels_, cmap='tab20', s=10)\nplt.title(\"TrackGNN Embedding Space with DBSCAN Clustering\")\nplt.xlabel(\"t-SNE 1\")\nplt.ylabel(\"t-SNE 2\")\nplt.colorbar(scatter, label=\"Cluster ID\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"embedding_tsne_plot.png\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.nn import global_mean_pool\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.neighbors import NearestNeighbors\nimport torch.nn.functional as F\n\n\n# 1. Define Model Architecture (Must match training exactly)\nclass EnhancedEdgeNetwork(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 128),\n            torch.nn.Tanh(),\n            torch.nn.Linear(128, 1),\n            torch.nn.Sigmoid()\n        )\n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 1)\n        )\n    \n    def forward(self, src, dest):\n        attn_weights = self.attention(torch.cat([src, dest], dim=-1))\n        return attn_weights * self.edge_mlp(torch.cat([src, dest], dim=-1))\n\nclass TrackGNN(torch.nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=512, embed_dim=256, n_iter=3):\n        super().__init__()\n        self.n_iter = n_iter\n        \n        # Original encoder components\n        self.convs = torch.nn.ModuleList([\n            GCNConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.edge_nets = torch.nn.ModuleList([\n            EnhancedEdgeNetwork(input_dim if i == 0 else hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.res_lin = torch.nn.ModuleList([\n            torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.bns = torch.nn.ModuleList([\n            torch.nn.BatchNorm1d(hidden_dim) for _ in range(n_iter)\n        ])\n        \n        self.lin = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, embed_dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.3)\n        )\n\n        # Add missing classifier with exact architecture from training\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(2*embed_dim, 512),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(512),\n            torch.nn.Dropout(0.2),\n            torch.nn.Linear(512, 256),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(256),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, data):\n        # Keep original forward logic\n        x, edge_index = data.x, data.edge_index\n        edge_row, edge_col = edge_index\n        \n        for i in range(self.n_iter):\n            edge_weights = torch.sigmoid(\n                self.edge_nets[i](x[edge_row], x[edge_col])\n            ).squeeze()\n            \n            x_new = F.relu(self.bns[i](self.convs[i](x, edge_index, edge_weights)))\n            x = x_new + self.res_lin[i](x)\n            \n        return global_mean_pool(self.lin(x), data.batch)\n\n\n# 2. Dataset Class for Inference\nfrom torch_geometric.data import Dataset\n\nclass GraphDataset(Dataset):\n    def __init__(self, graph_dir):\n        super().__init__()\n        self.graph_dir = graph_dir\n        self.file_list = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pkl')])\n        self.hit_ids = [self._extract_hit_id(f) for f in self.file_list]  # Store hit IDs\n        self.cache = {}\n\n    def _extract_hit_id(self, filename):\n        \"\"\"Extract HitID from filename (e.g., Graph_12345.pkl -> 12345)\"\"\"\n        return int(filename.split('_')[1].split('.')[0])\n\n    def load_graph(self, path):\n        if path not in self.cache:\n            with open(path, 'rb') as f:\n                G = pickle.load(f)\n            \n            nodes = list(G.nodes())\n            node_id_to_idx = {n: i for i, n in enumerate(nodes)}\n            \n            edge_list = []\n            seen_edges = set()\n            for u, v in G.edges():\n                src = node_id_to_idx[u]\n                dst = node_id_to_idx[v]\n                if (src, dst) not in seen_edges and (dst, src) not in seen_edges:\n                    edge_list.append([src, dst])\n                    seen_edges.add((src, dst))\n            \n            self.cache[path] = Data(\n                x=torch.tensor([G.nodes[n]['features'] for n in nodes], dtype=torch.float32),\n                edge_index=torch.tensor(edge_list).t().contiguous(),\n                num_nodes=len(nodes)\n            )\n        return self.cache[path]\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        return self.load_graph(os.path.join(self.graph_dir, self.file_list[idx]))\n\n    def get_hit_ids(self):\n        \"\"\"Get all HitIDs in dataset order\"\"\"\n        return self.hit_ids\n\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch_geometric.data import Data, DataLoader\n\n\ndef merge_tracks(model_path, test_graph_dir, output_csv):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load original data with validation\n    processed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n    if 'muonID' not in processed_data.columns:\n        raise ValueError(\"Processed data missing 'muonID' column\")\n    original_tracks = processed_data['muonID'].nunique()\n    \n    # Load model with safety checks\n    checkpoint = torch.load(model_path, map_location=device)\n    model = TrackGNN(\n        input_dim=checkpoint['config'].get('input_dim', 4),\n        hidden_dim=checkpoint['config'].get('hidden_dim', 512),\n        embed_dim=checkpoint['config'].get('embed_dim', 256)\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state'], strict=False)\n    model.eval()\n\n    # Batch processing setup\n    dataset = GraphDataset(test_graph_dir)\n    hit_ids = dataset.hit_ids\n    \n    loader = DataLoader(\n        dataset,\n        batch_size=64,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    # Process graphs in batches\n    embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Processing graphs\"):\n            batch = batch.to(device)\n            embeddings.append(model(batch).cpu().numpy())\n    \n    X = np.concatenate(embeddings)\n    X = StandardScaler().fit_transform(X)\n\n    # Density-based clustering with updated parameters\n    clusterer = DBSCAN(\n        eps=0.007,        # Increased epsilon\n        min_samples=2,   # Reduced minimum points\n        metric='cosine',\n        n_jobs=-1\n    ).fit(X)\n    \n    # Create results without size constraints\n    results = pd.DataFrame({\n        'HitID': hit_ids,\n        'TrackID': clusterer.labels_,\n    }).merge(\n        processed_data[['HitID', 'muonID']],\n        on='HitID',\n        how='left'\n    )\n\n    # Cluster validation without size limits\n    cluster_stats = results[results['TrackID'] != -1].groupby('TrackID').agg(\n        size=('HitID', 'count'),\n        purity=('muonID', lambda x: x.value_counts(normalize=True).iloc[0])\n    ).reset_index()\n\n    # Remove all size constraints\n    min_size = 1  # Absolute minimum\n    min_purity = 0.65  # Relaxed purity threshold\n\n    # Final validation (only purity check)\n    valid_tracks = cluster_stats[\n        (cluster_stats['purity'] >= min_purity)\n    ]\n\n    # Efficiency calculation\n    if valid_tracks.empty:\n        efficiency = 0.0\n    else:\n        efficiency = valid_tracks.merge(\n            results[['TrackID', 'muonID']].drop_duplicates(),\n            on='TrackID',\n            how='left'\n        )['muonID'].nunique() / original_tracks\n\n    # Diagnostic reporting\n    print(\"\\n=== Clustering Diagnostics ===\")\n    print(f\"Total clusters: {len(cluster_stats)}\")\n    print(f\"Noise points: {(results['TrackID'] == -1).sum()} ({(results['TrackID'] == -1).mean():.1%})\")\n    print(f\"Cluster size range: {cluster_stats['size'].min()}-{cluster_stats['size'].max()}\")\n    print(f\"Validation thresholds: purity ≥ {min_purity:.2f}\")\n    \n    print(\"\\n=== Final Results ===\")\n    print(f\"Original muon tracks: {original_tracks}\")\n    print(f\"Valid reconstructed tracks: {len(valid_tracks)}\")\n    print(f\"Tracking efficiency: {efficiency:.2%}\")\n\n    results.to_csv(output_csv, index=False)\n    return results, valid_tracks, efficiency\n\n\n\nif __name__ == \"__main__\":\n    results, stats, efficiency = merge_tracks(\n        model_path=\"/kaggle/input/new-set/track_gnn_best.pt\",\n        test_graph_dir=\"/kaggle/working/graphs1\",\n        output_csv=\"merged_tracks.csv\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.nn import global_mean_pool\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.neighbors import NearestNeighbors\nimport torch.nn.functional as F\n\n\n# 1. Define Model Architecture (Must match training exactly)\nclass EnhancedEdgeNetwork(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 128),\n            torch.nn.Tanh(),\n            torch.nn.Linear(128, 1),\n            torch.nn.Sigmoid()\n        )\n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 1)\n        )\n    \n    def forward(self, src, dest):\n        attn_weights = self.attention(torch.cat([src, dest], dim=-1))\n        return attn_weights * self.edge_mlp(torch.cat([src, dest], dim=-1))\n\nclass TrackGNN(torch.nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=512, embed_dim=256, n_iter=3):\n        super().__init__()\n        self.n_iter = n_iter\n        \n        # Original encoder components\n        self.convs = torch.nn.ModuleList([\n            GCNConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.edge_nets = torch.nn.ModuleList([\n            EnhancedEdgeNetwork(input_dim if i == 0 else hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.res_lin = torch.nn.ModuleList([\n            torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        \n        self.bns = torch.nn.ModuleList([\n            torch.nn.BatchNorm1d(hidden_dim) for _ in range(n_iter)\n        ])\n        \n        self.lin = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, embed_dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.3)\n        )\n\n        # Add missing classifier with exact architecture from training\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(2*embed_dim, 512),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(512),\n            torch.nn.Dropout(0.2),\n            torch.nn.Linear(512, 256),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(256),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, data):\n        # Keep original forward logic\n        x, edge_index = data.x, data.edge_index\n        edge_row, edge_col = edge_index\n        \n        for i in range(self.n_iter):\n            edge_weights = torch.sigmoid(\n                self.edge_nets[i](x[edge_row], x[edge_col])\n            ).squeeze()\n            \n            x_new = F.relu(self.bns[i](self.convs[i](x, edge_index, edge_weights)))\n            x = x_new + self.res_lin[i](x)\n            \n        return global_mean_pool(self.lin(x), data.batch)\n\n\n# 2. Dataset Class for Inference\nfrom torch_geometric.data import Dataset\n\nclass GraphDataset(Dataset):\n    def __init__(self, graph_dir):\n        super().__init__()\n        self.graph_dir = graph_dir\n        self.file_list = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pkl')])\n        self.hit_ids = [self._extract_hit_id(f) for f in self.file_list]  # Store hit IDs\n        self.cache = {}\n\n    def _extract_hit_id(self, filename):\n        \"\"\"Extract HitID from filename (e.g., Graph_12345.pkl -> 12345)\"\"\"\n        return int(filename.split('_')[1].split('.')[0])\n\n    def load_graph(self, path):\n        if path not in self.cache:\n            with open(path, 'rb') as f:\n                G = pickle.load(f)\n            \n            nodes = list(G.nodes())\n            node_id_to_idx = {n: i for i, n in enumerate(nodes)}\n            \n            edge_list = []\n            seen_edges = set()\n            for u, v in G.edges():\n                src = node_id_to_idx[u]\n                dst = node_id_to_idx[v]\n                if (src, dst) not in seen_edges and (dst, src) not in seen_edges:\n                    edge_list.append([src, dst])\n                    seen_edges.add((src, dst))\n            \n            self.cache[path] = Data(\n                x=torch.tensor([G.nodes[n]['features'] for n in nodes], dtype=torch.float32),\n                edge_index=torch.tensor(edge_list).t().contiguous(),\n                num_nodes=len(nodes)\n            )\n        return self.cache[path]\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        return self.load_graph(os.path.join(self.graph_dir, self.file_list[idx]))\n\n    def get_hit_ids(self):\n        \"\"\"Get all HitIDs in dataset order\"\"\"\n        return self.hit_ids\n\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch_geometric.data import Data, DataLoader\n\n\ndef merge_tracks(model_path, test_graph_dir, output_csv):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load original data with validation\n    processed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n    if 'muonID' not in processed_data.columns:\n        raise ValueError(\"Processed data missing 'muonID' column\")\n    original_tracks = processed_data['muonID'].nunique()\n    \n    # Load model with safety checks\n    checkpoint = torch.load(model_path, map_location=device)\n    model = TrackGNN(\n        input_dim=checkpoint['config'].get('input_dim', 4),\n        hidden_dim=checkpoint['config'].get('hidden_dim', 512),\n        embed_dim=checkpoint['config'].get('embed_dim', 256)\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state'], strict=False)\n    model.eval()\n\n    # Batch processing setup\n    dataset = GraphDataset(test_graph_dir)\n    hit_ids = dataset.hit_ids\n    \n    loader = DataLoader(\n        dataset,\n        batch_size=64,\n        num_workers=4,\n        pin_memory=True\n    )\n\n    # Process graphs in batches\n    embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Processing graphs\"):\n            batch = batch.to(device)\n            embeddings.append(model(batch).cpu().numpy())\n    \n    X = np.concatenate(embeddings)\n    X = StandardScaler().fit_transform(X)\n\n    # k-NN based track merging\n    k = 3  # Number of nearest neighbors\n    neighbor_model = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n    neighbor_model.fit(X)\n\n    # Find k nearest neighbors for each track\n    distances, indices = neighbor_model.kneighbors(X)\n    \n    # Initialize track assignments\n    track_labels = -1 * np.ones(X.shape[0], dtype=int)\n    cluster_id = 0\n    \n    # Assign clusters based on nearest neighbors\n    for i in range(len(X)):\n        if track_labels[i] == -1:  # If not yet assigned\n            # Assign the current node and its neighbors to the same track\n            track_labels[i] = cluster_id\n            for neighbor_idx in indices[i]:\n                if track_labels[neighbor_idx] == -1:\n                    track_labels[neighbor_idx] = cluster_id\n            cluster_id += 1\n\n    # Create results with physics constraints\n    results = pd.DataFrame({\n        'HitID': hit_ids,\n        'TrackID': track_labels,\n    }).merge(\n        processed_data[['HitID', 'muonID']],\n        on='HitID',\n        how='left'\n    )\n\n    # Robust cluster validation\n    cluster_stats = results[results['TrackID'] != -1].groupby('TrackID').agg(\n        size=('HitID', 'count'),\n        purity=('muonID', lambda x: x.value_counts(normalize=True).iloc[0])\n    ).reset_index()\n\n    # Handle empty/no clusters scenario\n    if cluster_stats.empty:\n        print(\"⚠️ No clusters found - all points classified as noise\")\n        return pd.DataFrame(), pd.DataFrame(), 0.0\n\n    # Physics-based filtering (only purity)\n    valid_tracks = cluster_stats[cluster_stats['purity'] >= 0.7]  # Fixed 70% purity threshold\n    \n    # Noise calculation\n    noise_points = (results['TrackID'] == -1).sum()\n    noise_ratio = noise_points / len(results)\n\n    # Safe efficiency calculation\n    if valid_tracks.empty:\n        efficiency = 0.0\n    else:\n        efficiency_df = valid_tracks.merge(\n            results[['TrackID', 'muonID']].drop_duplicates(),\n            on='TrackID',\n            how='left'\n        )\n        efficiency = efficiency_df['muonID'].nunique() / original_tracks\n\n    # Diagnostic reporting\n    print(\"\\n=== Clustering Diagnostics ===\")\n    print(f\"Total clusters: {len(cluster_stats)}\")\n    print(f\"Noise points: {noise_points} ({noise_ratio:.1%})\")\n    print(f\"Cluster purity range: {cluster_stats['purity'].min():.2f}-{cluster_stats['purity'].max():.2f}\")\n    \n    print(\"\\n=== Final Results ===\")\n    print(f\"Original muon tracks: {original_tracks}\")\n    print(f\"Valid reconstructed tracks (purity ≥70%): {len(valid_tracks)}\")\n    print(f\"Tracking efficiency: {efficiency:.2%}\")\n\n    results.to_csv(output_csv, index=False)\n    return results, valid_tracks, efficiency\n\n\n\nif __name__ == \"__main__\":\n    results, stats, efficiency = merge_tracks(\n        model_path=\"/kaggle/input/new-set/track_gnn_maxhit.pt\",\n        test_graph_dir=\"/kaggle/working/graphs1\",\n        output_csv=\"merged_tracks.csv\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.loader import DataLoader  # Corrected import\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch.nn.functional as F\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.neighbors import NearestNeighbors\n\n# 1. Define Model Architecture with Edge Handling\nclass EnhancedEdgeNetwork(torch.nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.attention = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 128),\n            torch.nn.Tanh(),\n            torch.nn.Linear(128, 1),\n            torch.nn.Sigmoid()\n        )\n        self.edge_mlp = torch.nn.Sequential(\n            torch.nn.Linear(2*input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 1)\n        )\n    \n    def forward(self, src, dest):\n        attn_weights = self.attention(torch.cat([src, dest], dim=-1))\n        return attn_weights * self.edge_mlp(torch.cat([src, dest], dim=-1))\n\nclass TrackGNN(torch.nn.Module):\n    def __init__(self, input_dim=4, hidden_dim=512, embed_dim=256, n_iter=3):\n        super().__init__()\n        self.n_iter = n_iter\n        self.convs = torch.nn.ModuleList([\n            GCNConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        self.edge_nets = torch.nn.ModuleList([\n            EnhancedEdgeNetwork(input_dim if i == 0 else hidden_dim)\n            for i in range(n_iter)\n        ])\n        self.res_lin = torch.nn.ModuleList([\n            torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(n_iter)\n        ])\n        self.bns = torch.nn.ModuleList([\n            torch.nn.BatchNorm1d(hidden_dim) for _ in range(n_iter)\n        ])\n        self.lin = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim, embed_dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.3)\n        )\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(2*embed_dim, 512),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(512),\n            torch.nn.Dropout(0.2),\n            torch.nn.Linear(512, 256),\n            torch.nn.ReLU(),\n            torch.nn.LayerNorm(256),\n            torch.nn.Linear(256, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        \n        # Handle empty edge indices\n        if edge_index.size(1) == 0:\n            # Process isolated nodes directly\n            for i in range(self.n_iter):\n                x = F.relu(self.bns[i](self.res_lin[i](x)))\n            return global_mean_pool(self.lin(x), data.batch)\n        \n        edge_row, edge_col = edge_index\n        for i in range(self.n_iter):\n            edge_weights = torch.sigmoid(\n                self.edge_nets[i](x[edge_row], x[edge_col])\n            ).squeeze()\n            x_new = F.relu(self.bns[i](self.convs[i](x, edge_index, edge_weights)))\n            x = x_new + self.res_lin[i](x)\n            \n        return global_mean_pool(self.lin(x), data.batch)\n\n# 2. Dataset Class with Edge Handling\nclass GraphDataset(Dataset):\n    def __init__(self, graph_dir):\n        super().__init__()\n        self.graph_dir = graph_dir\n        self.file_list = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pkl')])\n        self.hit_ids = [int(f.split('_')[1].split('.')[0]) for f in self.file_list]\n        self.cache = {}\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.graph_dir, self.file_list[idx])\n        if path not in self.cache:\n            with open(path, 'rb') as f:\n                G = pickle.load(f)\n            \n            nodes = list(G.nodes())\n            features = np.array([G.nodes[n]['features'] for n in nodes])  # Convert to numpy first\n            edge_list = []\n            \n            # Handle edge indices\n            for u, v in G.edges():\n                src = nodes.index(u)\n                dst = nodes.index(v)\n                if src != dst:  # Avoid self-loops\n                    edge_list.append([src, dst])\n                    edge_list.append([dst, src])  # Add reverse direction\n\n            self.cache[path] = Data(\n                x=torch.tensor(features, dtype=torch.float32),  # Fixed tensor creation\n                edge_index=torch.tensor(edge_list).t().contiguous() if edge_list else torch.empty((2, 0), dtype=torch.long),\n                num_nodes=len(nodes)\n            )\n        return self.cache[path]\n\n    def get_hit_ids(self):\n        return self.hit_ids\n\n# 3. Main Processing Function\ndef merge_tracks(model_path, test_graph_dir, output_csv):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # 1. Enhanced Data Loading with Column Verification\n    processed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n    \n    # Verify column names exist with exact case matching\n    required_columns = {'HitID', 'muonID'}\n    missing_cols = required_columns - set(processed_data.columns)\n    if missing_cols:\n        raise KeyError(f\"Missing critical columns in processed data: {missing_cols}\")\n\n    # 2. Model Loading with Architecture Verification\n    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n    model = TrackGNN(\n        input_dim=checkpoint.get('input_dim', 4),\n        hidden_dim=checkpoint.get('hidden_dim', 512),\n        embed_dim=checkpoint.get('embed_dim', 256)\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state'], strict=True)  # Enforce exact parameter match\n    model.eval()\n\n    # 3. Data Pipeline with Column Sanity Checks\n    dataset = GraphDataset(test_graph_dir)\n    loader = DataLoader(dataset, batch_size=64, num_workers=4, shuffle=False)\n\n    # 4. Embedding Generation with Dimension Verification\n    embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Processing graphs\"):\n            batch = batch.to(device)\n            emb = model(batch)\n            if emb.shape[1] != 256:  # Verify embedding dimension matches clustering needs\n                raise ValueError(f\"Unexpected embedding dimension: {emb.shape}\")\n            embeddings.append(emb.cpu().numpy())\n    \n    # 5. Clustering with Adaptive Parameters\n    X = np.concatenate(embeddings)\n    X = StandardScaler().fit_transform(X)\n    \n    # Dynamic epsilon calculation with bounds\n    eps_percentile = np.clip(np.percentile(pdist(X), 0.000001), 1e-6, 0.1)\n    dbscan = DBSCAN(\n        eps=eps_percentile,\n        min_samples=3,  # Increased from 3 for more stable clusters\n        metric='cosine',\n        n_jobs=-1\n    )\n    track_labels = dbscan.fit_predict(X)\n    \n    # 6. Result Compilation with Column Validation\n    results = pd.DataFrame({\n        'HitID': dataset.hit_ids,\n        'TrackID': track_labels\n    }).merge(\n        processed_data[['HitID', 'muonID']],\n        on='HitID',\n        how='inner'\n    )\n    \n    # Post-merge column verification\n    if 'muonID' not in results.columns:\n        missing_hits = len(dataset.hit_ids) - len(results)\n        raise KeyError(f\"muonID missing after merge ({missing_hits} hits lost)\")\n\n    # 7. Enhanced Physics Validation\n    original_tracks = processed_data['muonID'].nunique()\n    \n    # Calculate noise statistics\n    noise_points = (results['TrackID'] == -1).sum()\n    noise_ratio = noise_points / len(results)\n    \n    # Cluster analysis\n    valid_clusters = []\n    cluster_stats = []\n    \n    if not results.empty:\n        for track_id, group in results[results['TrackID'] != -1].groupby('TrackID'):\n            muon_counts = group['muonID'].value_counts(normalize=True)\n            if not muon_counts.empty:\n                stats = {\n                    'TrackID': track_id,\n                    'size': len(group),\n                    'purity': muon_counts.iloc[0],\n                    'main_muon': muon_counts.idxmax()\n                }\n                cluster_stats.append(stats)\n                \n                # Apply physics criteria\n                if stats['purity'] >= 0.7 and stats['size'] >= 5:\n                    valid_clusters.append(stats)\n\n    # Efficiency calculation\n    valid_muons = {c['main_muon'] for c in valid_clusters}\n    correctly_reconstructed = len(valid_muons)\n    efficiency = correctly_reconstructed / original_tracks if original_tracks > 0 else 0\n\n    # 8. Comprehensive Diagnostics\n    print(\"\\n=== Pipeline Diagnostics ===\")\n    print(f\"Total input hits: {len(dataset)}\")\n    print(f\"Original muon tracks: {original_tracks}\")\n    print(f\"Reconstructed clusters: {results['TrackID'].nunique() - 1}\")  # Exclude noise\n    print(f\"Noise points: {noise_points} ({noise_ratio:.2%})\")\n    print(f\"Valid physics clusters: {len(valid_clusters)}\")\n    print(f\"Unique muons recovered: {correctly_reconstructed}\")\n    print(f\"Tracking efficiency: {efficiency:.2%}\")\n    \n    if cluster_stats:\n        sizes = [c['size'] for c in cluster_stats]\n        purities = [c['purity'] for c in cluster_stats]\n        print(f\"\\nAll clusters size range: {min(sizes)}-{max(sizes)}\")\n        print(f\"All clusters purity range: {min(purities):.2f}-{max(purities):.2f}\")\n        \n    if valid_clusters:\n        valid_sizes = [c['size'] for c in valid_clusters]\n        valid_purities = [c['purity'] for c in valid_clusters]\n        print(f\"\\nValid clusters size range: {min(valid_sizes)}-{max(valid_sizes)}\")\n        print(f\"Valid clusters avg purity: {np.mean(valid_purities):.2f}\")\n    else:\n        print(\"\\nNo valid clusters meeting physics criteria\")\n\n    results.to_csv(output_csv, index=False)\n    return results, efficiency\n\nif __name__ == \"__main__\":\n    results, efficiency = merge_tracks(\n        model_path=\"/kaggle/working/saved_models/track_gnn_20250402-100645.pt\",\n        test_graph_dir=\"/kaggle/working/graphs\",\n        output_csv=\"merged_tracks.csv\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs = pd.read_csv('/kaggle/working/merged_tracks.csv')\ngraphs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef merge_tracks_and_evaluate(graph_dir, metadata_csv, output_csv):\n    # Load metadata\n    metadata = pd.read_csv(metadata_csv)\n    hitid_to_muon = dict(zip(metadata['HitID'], metadata['muonID']))\n\n    # ✅ Count total true tracks\n    total_true_tracks = metadata['muonID'].nunique()\n\n    # Get all graph files\n    graph_files = [os.path.join(graph_dir, f) for f in os.listdir(graph_dir) if f.endswith('.pkl')]\n\n    # Generate embeddings for all graphs\n    embeddings = {}\n    hit_ids = []\n\n    for graph_file in tqdm(graph_files, desc=\"Processing graphs\"):\n        # Extract HitID from filename\n        hit_id = int(os.path.basename(graph_file).split('_')[1].split('.')[0])\n        hit_ids.append(hit_id)\n\n        # Load and process graph\n        with open(graph_file, 'rb') as f:\n            G = pickle.load(f)\n\n        # Store random 2D embeddings for DBSCAN clustering\n        embeddings[hit_id] = np.random.rand(2)  \n\n    # Convert to numpy arrays\n    hit_ids = np.array(hit_ids)\n    emb_array = np.array([embeddings[hid] for hid in hit_ids])\n\n    # Cluster embeddings\n    clustering = DBSCAN(eps=0.0001, min_samples=2).fit(emb_array)\n    labels = clustering.labels_\n\n    # Create result dataframe\n    result_df = pd.DataFrame({\n        'HitID': hit_ids,\n        'PredictedTrack': labels,\n        'TrueMuonID': [hitid_to_muon[hid] for hid in hit_ids]\n    })\n\n    # ✅ Sort by `TrueMuonID`\n    result_df.sort_values(by='TrueMuonID', ascending=True, inplace=True)\n\n    # ✅ Purity-based Efficiency Calculation\n    reconstructed_tracks = 0\n    purity_threshold = 0.7\n    purity_data = []\n\n    for true_muon in result_df['TrueMuonID'].unique():\n        muon_hits = result_df[result_df['TrueMuonID'] == true_muon]\n\n        # Count the hits per predicted track\n        cluster_counts = muon_hits['PredictedTrack'].value_counts()\n        \n        if len(cluster_counts) == 0:\n            continue\n\n        # Identify the majority cluster\n        main_cluster = cluster_counts.idxmax()\n        correct_hits = cluster_counts[main_cluster]\n        total_hits = len(muon_hits)\n\n        # Calculate purity\n        purity = correct_hits / total_hits\n\n        # Only consider tracks with ≥ 70% purity\n        if purity >= purity_threshold:\n            reconstructed_tracks += 1\n\n        # Store purity data\n        purity_data.append({\n            'TrueMuonID': true_muon,\n            'MajorityCluster': main_cluster,\n            'TotalHits': total_hits,\n            'CorrectHits': correct_hits,\n            'Purity': purity\n        })\n\n    # ✅ Calculate Overall Efficiency\n    overall_efficiency = reconstructed_tracks / total_true_tracks\n\n    print(f\"\\nOverall Tracking Efficiency: {overall_efficiency:.2%}\")\n    print(f\"Number of Reconstructed Tracks (≥ 70% purity): {reconstructed_tracks}\")\n    print(f\"Total Number of True Tracks: {total_true_tracks}\")\n    print(f\"Number of Noise Points: {sum(labels == -1)}\")\n\n    # Save results\n    result_df.to_csv(output_csv, index=False)\n\n    # Save purity data\n    purity_df = pd.DataFrame(purity_data)\n    purity_df.to_csv(output_csv.replace('.csv', '_purity.csv'), index=False)\n\n    # Save efficiency summary\n    efficiency_summary = {\n        'TotalTrueTracks': total_true_tracks,\n        'ReconstructedTracks': reconstructed_tracks,\n        'Efficiency': overall_efficiency\n    }\n\n    efficiency_df = pd.DataFrame([efficiency_summary])\n    efficiency_df.to_csv(output_csv.replace('.csv', '_efficiency.csv'), index=False)\n\n    return result_df, efficiency_df, purity_df\n\n# ✅ Usage\nif __name__ == \"__main__\":\n    results, efficiency, purity = merge_tracks_and_evaluate(\n        graph_dir=\"graphs\",\n        metadata_csv=\"/kaggle/working/processed_data_with_hitid.csv\",\n        output_csv=\"/kaggle/working/track_assignments.csv\"\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs = pd.read_csv('/kaggle/working/merged_tracks.csv')\ngraphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Positive samples: {sum(labels)}/{len(labels)}\")\nprint(f\"Negative samples: {len(labels)-sum(labels)}/{len(labels)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.5.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.5.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nprint(torch.version.cuda)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom sklearn.cluster import DBSCAN\nfrom scipy.spatial import cKDTree\nfrom collections import defaultdict\nimport networkx as nx\n\n# Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ngraphs_dir = '/kaggle/working/graphs1'\ntrack_hits_file = '/kaggle/working/track_hits1.csv'\nos.makedirs(graphs_dir, exist_ok=True)\n\n# 1. Data Preparation\ndef prepare_data(processed_data):\n    features = ['pos_x', 'pos_y', 'pos_z', 'time']\n    normalized_data = processed_data.copy()\n    for feat in features:\n        min_val = processed_data[feat].min()\n        max_val = processed_data[feat].max()\n        normalized_data[feat] = (processed_data[feat] - min_val) / (max_val - min_val)\n    return normalized_data\n\n# 2. Global Graph Construction\nclass GlobalGraphBuilder:\n    def __init__(self):\n        self.global_graph = nx.Graph()\n        self.node_mapping = {}\n        self.current_idx = 0\n        \n    def add_subgraphs(self, graphs):\n        \"\"\"Add subgraphs with empty check\"\"\"\n        if not graphs:\n            raise ValueError(\"No subgraphs provided\")\n            \n        for G in graphs:\n            if len(G.nodes) == 0:\n                print(\"Warning: Skipped empty subgraph\")\n                continue\n        \"\"\"Add subgraphs with scalar feature validation\"\"\"\n        for G in graphs:\n            for node, attrs in G.nodes(data=True):\n                # Convert to scalar floats\n                validated_attrs = {\n                    'pos_x': float(attrs['pos_x']),\n                    'pos_y': float(attrs['pos_y']),\n                    'pos_z': float(attrs['pos_z']),\n                    'time': float(attrs['time'])\n                }\n                if node not in self.node_mapping:\n                    self.node_mapping[node] = self.current_idx\n                    self.global_graph.add_node(self.current_idx, **validated_attrs)\n                    self.current_idx += 1\n            for u, v in G.edges():\n                self.global_graph.add_edge(\n                    self.node_mapping[u],\n                    self.node_mapping[v],\n                    edge_type='intra'\n                )\n                \n    def add_cross_edges(self, spatial_thresh=0.1, time_thresh=0.2):\n        \"\"\"Propose cross-subgraph edges with dimension safety\"\"\"\n        nodes = []\n        for n in self.global_graph.nodes():\n            attrs = self.global_graph.nodes[n]\n            nodes.append([\n                attrs['pos_x'], attrs['pos_y'], attrs['pos_z'], attrs['time']\n            ])\n        \n        # Handle empty and 1D cases\n        if not nodes:\n            return\n        nodes = np.array(nodes)\n        if nodes.ndim == 1:\n            nodes = nodes.reshape(1, -1)\n            \n        # Spatial-temporal proximity\n        spatial_tree = cKDTree(nodes[:, :3])\n        time_tree = cKDTree(nodes[:, 3].reshape(-1,1))\n        \n        spatial_pairs = spatial_tree.query_pairs(spatial_thresh)\n        time_pairs = time_tree.query_pairs(time_thresh)\n        valid_pairs = spatial_pairs.intersection(time_pairs)\n        \n        # Add edges\n        for i, j in valid_pairs:\n            if not self.global_graph.has_edge(i, j):\n                self.global_graph.add_edge(i, j, edge_type='cross')\n\n# 3. GNN Architecture\nclass TrackGNN(nn.Module):\n    def __init__(self, node_dim=4, hidden_dim=64):\n        super().__init__()\n        self.conv1 = GATConv(node_dim, hidden_dim)\n        self.conv2 = GATConv(hidden_dim, hidden_dim)\n        self.edge_classifier = nn.Sequential(\n            nn.Linear(2*hidden_dim + 4, hidden_dim),  # 4 = dx, dy, dz, dt\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        \n        # Dimension validation\n        if x.dim() != 2:\n            raise ValueError(f\"Input features must be 2D (got {x.dim()}D)\")\n            \n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        \n        # Edge features\n        row, col = edge_index\n        dx = data.x[row, 0] - data.x[col, 0]\n        dy = data.x[row, 1] - data.x[col, 1]\n        dz = data.x[row, 2] - data.x[col, 2]\n        dt = data.x[row, 3] - data.x[col, 3]\n        edge_feats = torch.stack([dx, dy, dz, dt], dim=1)\n        \n        # Classification\n        edge_features = torch.cat([x[row], x[col], edge_feats], dim=1)\n        return self.edge_classifier(edge_features).squeeze()\n\n# 4. Training and Inference\nclass TrackMerger:\n    def __init__(self, processor):\n        self.processor = processor\n        self.model = TrackGNN().to(device)\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n    \n    def prepare_pyg_data(self, global_graph):\n        \"\"\"Create PyG data with empty check and dimension safety\"\"\"\n        node_ids = sorted(global_graph.nodes())\n        \n        # Handle empty graph\n        if not node_ids:\n            raise ValueError(\"Global graph contains no nodes. Check input subgraphs.\")\n            \n        # Collect node features\n        node_feats = []\n        for nid in node_ids:\n            attrs = global_graph.nodes[nid]\n            node_feats.append([\n                float(attrs['pos_x']),\n                float(attrs['pos_y']),\n                float(attrs['pos_z']),\n                float(attrs['time'])\n            ])\n        \n        # Convert to tensor and ensure 2D\n        x = torch.tensor(node_feats, dtype=torch.float32).to(device)\n        if x.dim() == 1:\n            x = x.unsqueeze(0)  # Convert [4] -> [1, 4]\n            \n        print(f\"Node features shape: {x.shape}\")  # Debug\n        \n        # Edge index handling\n        edge_list = list(global_graph.edges())\n        if edge_list:\n            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous().to(device)\n        else:\n            edge_index = torch.empty((2, 0), dtype=torch.long).to(device)\n        \n        return Data(x=x, edge_index=edge_index)\n    \n    def train(self, data, epochs=50):\n        self.model.train()\n        for epoch in range(epochs):\n            self.optimizer.zero_grad()\n            preds = self.model(data)\n            loss = F.binary_cross_entropy(preds, torch.ones_like(preds))  # Assuming all edges are valid\n            loss.backward()\n            self.optimizer.step()\n            print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n    \n    def predict_edges(self, data):\n        self.model.eval()\n        with torch.no_grad():\n            return self.model(data).cpu().numpy()\n\n# 5. Global Clustering\ndef merge_tracks(global_graph, edge_scores, eps=0.1):\n    node_ids = list(global_graph.nodes())\n    distance_matrix = np.zeros((len(node_ids), len(node_ids)))\n    \n    for (u, v), score in edge_scores.items():\n        distance_matrix[u, v] = 1 - score\n        distance_matrix[v, u] = 1 - score\n    \n    clusters = DBSCAN(eps=eps, min_samples=3, \n                     metric='precomputed').fit_predict(distance_matrix)\n    \n    tracks = {}\n    for cluster_id in np.unique(clusters):\n        if cluster_id == -1: continue\n        indices = np.where(clusters == cluster_id)[0]\n        tracks[cluster_id] = [node_ids[i] for i in indices]\n    \n    return tracks\n\n# Main Workflow\nif __name__ == \"__main__\":\n    # 1. Prepare data\n    processed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\n    processed_data = prepare_data(processed_data)\n    \n    # 2. Build global graph\n    graphs = [pickle.load(open(os.path.join(graphs_dir, f), 'rb')) \n             for f in os.listdir(graphs_dir) if f.endswith('.pkl')]\n    \n    graph_builder = GlobalGraphBuilder()\n    graph_builder.add_subgraphs(graphs)\n    graph_builder.add_cross_edges()\n    global_graph = graph_builder.global_graph\n    \n    # 3. Prepare GNN data\n    merger = TrackMerger(graph_builder)\n    pyg_data = merger.prepare_pyg_data(global_graph)\n    \n    # 4. Train model (or load pretrained)\n    merger.train(pyg_data)  # Uncomment to train\n    torch.save(merger.model.state_dict(), 'track_gnn.pth')\n    merger.model.load_state_dict(torch.load('track_gnn.pth', map_location=device))\n    \n    # 5. Predict edge scores\n    edge_scores = merger.predict_edges(pyg_data)\n    edge_dict = {(u.item(), v.item()): score \n                for (u, v), score in zip(pyg_data.edge_index.t().tolist(), edge_scores)}\n    \n    # 6. Merge tracks\n    tracks = merge_tracks(global_graph, edge_dict)\n    \n    # 7. Save results\n    track_hits = []\n    reverse_mapping = {v: k for k, v in graph_builder.node_mapping.items()}\n    for track_id, nodes in tracks.items():\n        original_nodes = [reverse_mapping[n] for n in nodes]\n        track_hits.append({\n            'track_id': track_id,\n            'nodes': original_nodes,\n            'num_points': len(nodes)\n        })\n    \n    pd.DataFrame(track_hits).to_csv(track_hits_file, index=False)\n    print(f\"Saved {len(tracks)} merged tracks to {track_hits_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs_dir = '/kaggle/working/graphs'\nupdated_graphs_dir = '/kaggle/working/updated_final1'\ntrack_hits_file = '/kaggle/working/track_hits1.csv'\ngraphs_hits_file = '/kaggle/working/graphs_hits.csv'\nos.makedirs(updated_graphs_dir, exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfeatures = ['pos_z', 'time']\nfor feature in features:\n    min_value = processed_data[feature].min()\n    max_value = processed_data[feature].max()\n    processed_data[feature] = (processed_data[feature] - min_value) / (max_value - min_value)\n\nprint(\"Min-max normalization applied to features:\", features)\n\ndef load_graphs(graphs_dir):\n    graphs = []\n    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pkl')]\n    for graph_file in graph_files:\n        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n            G = pickle.load(f)\n            graphs.append(G)\n    return graphs\n\ngraphs = load_graphs(graphs_dir)\n\ndef save_graph_with_scores(graph, updated_graphs_dir, edge_scores, graph_name):\n    graph_file = f'{graph_name}_with_scores.pkl'\n    for (u, v), score in edge_scores.items():\n        if graph.has_edge(u, v):\n            graph[u][v]['score'] = score\n    with open(os.path.join(updated_graphs_dir, graph_file), 'wb') as f:\n        pickle.dump(graph, f)\n\nclass EdgeNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(EdgeNetwork, self).__init__()\n        self.edge_mlp = nn.Sequential(\n            nn.Linear(2 * input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    def forward(self, hi, hj):\n        concatenated = torch.cat((hi, hj), dim=-1)\n        return self.edge_mlp(concatenated)\n\nclass NodeNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(NodeNetwork, self).__init__()\n        self.node_mlp = nn.Sequential(\n            nn.Linear(3 * input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    def forward(self, h_in, s_in, h_out):\n        return self.node_mlp(torch.cat((s_in * h_in, s_in * h_out, h_in), dim=-1)) + h_in\n\nclass GNN(nn.Module):\n    def __init__(self, edge_in_dim, edge_hidden_dim, edge_out_dim,\n                 node_in_dim, node_hidden_dim, node_out_dim, n_iter):\n        super(GNN, self).__init__()\n        self.edge_network = EdgeNetwork(edge_in_dim, edge_hidden_dim, edge_out_dim)\n        self.node_network = NodeNetwork(node_in_dim, node_hidden_dim, node_out_dim)\n        self.n_iter = n_iter\n    def forward(self, node_features, edge_indices):\n        for _ in range(self.n_iter):\n            new_node_features = {}\n            for node_id, neighbors in edge_indices.items():\n                h_in = node_features[node_id]\n                s_in = []\n                for neighbor_id in neighbors:\n                    h_out = node_features[neighbor_id]\n                    s_in.append(self.edge_network(h_in, h_out))\n                if s_in:\n                    s_in = torch.stack(s_in, dim=0)\n                    new_node_features[node_id] = self.node_network(h_in, s_in.mean(dim=0), h_out)\n                else:\n                    new_node_features[node_id] = h_in\n            node_features = new_node_features\n        return node_features\n\nnode_input_dim = 4\nmodel = GNN(edge_in_dim=node_input_dim,\n            edge_hidden_dim=64,\n            edge_out_dim=1,\n            node_in_dim=node_input_dim,\n            node_hidden_dim=64,\n            node_out_dim=node_input_dim,\n            n_iter=2)\n\nmodel.to(device)\nmodel.load_state_dict(torch.load(model_gnn, map_location=device))\nmodel.eval()\nprint(\"Model loaded successfully.\")\n\nnode_features = {}\nfor index, row in processed_data.iterrows():\n    hit_id = row['HitID']\n    pos_x = row['pos_x']\n    pos_y = row['pos_y']\n    pos_z = row['pos_z']\n    time = row['time']\n    node_features[hit_id] = torch.tensor([pos_x, pos_y, pos_z, time], dtype=torch.float32)\n\nnode_features = {k: v.to(device) for k, v in node_features.items()}\nprint(\"Node features prepared successfully.\")\n\nhitid_to_muonid = processed_data.set_index('HitID')['muonID'].to_dict()\n\ndef create_distance_matrix(edge_scores, num_nodes):\n    rows = []\n    cols = []\n    data = []\n    \n    for (u, v), score in edge_scores.items():\n        if u < num_nodes and v < num_nodes:\n            rows.append(u)\n            cols.append(v)\n            data.append(1 - 2*score)  # distance = 1 - score\n    \n    distance_matrix = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n    distance_matrix = (distance_matrix + distance_matrix.T) / 2  # Symmetrize matrix\n    return distance_matrix\n\ndef apply_dbscan(distance_matrix, eps=0.005):\n    dbscan = DBSCAN(eps=eps, metric='precomputed', min_samples=1)\n    labels = dbscan.fit_predict(distance_matrix)\n    return labels\n\nall_edge_scores = {}\ntrack_hits = {}  # Dictionary to aggregate points and muonIDs by track label\ngraph_hits = []\n\nfor i, G in enumerate(graphs):\n    edge_indices = {node: list(G.neighbors(node)) for node in G.nodes}\n    outputs = model(node_features, edge_indices)\n    edge_scores = {}\n    for node_id, neighbors in edge_indices.items():\n        h_in = outputs[node_id]\n        for neighbor_id in neighbors:\n            h_out = outputs[neighbor_id]\n            edge_score = model.edge_network(h_in, h_out)\n            edge_score = torch.sigmoid(edge_score).item()\n            edge_scores[(node_id, neighbor_id)] = edge_score\n    all_edge_scores[i] = edge_scores\n    save_graph_with_scores(G, updated_graphs_dir, edge_scores, f'graph_{i + 1}')\n    \n    graph_hits.append({\n    'track_number': i + 1,\n    'points': list(G.nodes),\n    'muonIDs': [hitid_to_muonid[hit_id] for hit_id in G.nodes]\n    })\n    \n    num_nodes = len(G.nodes)\n    distance_matrix = create_distance_matrix(all_edge_scores[i], num_nodes)\n    labels = apply_dbscan(distance_matrix, eps=0.1)  # Adjust eps as needed\n    \n    # Aggregate points and muonIDs by track label\n    for j, node in enumerate(G.nodes):\n        label = labels[j]\n        if label not in track_hits:\n            track_hits[label] = {'points': [], 'muonIDs': []}\n        track_hits[label]['points'].append(node)\n        track_hits[label]['muonIDs'].append(hitid_to_muonid.get(node, None))\n\ntrack_hits_list = [{\n    'track_label': label,\n    'points': data['points'],\n    'muonIDs': data['muonIDs']\n} for label, data in track_hits.items()]\n\n# Print the first 5 edge scores\nprint(\"First 5 Edge Scores:\")\nfor i, edge_scores in list(all_edge_scores.items())[:5]:\n    print(f\"Graph {i + 1}:\")\n    for (node_pair, score) in list(edge_scores.items())[:5]:\n        print(f\"  Edge {node_pair}: Score {score}\")\n\n\ntrack_hits_df = pd.DataFrame(track_hits_list)\n\ntrack_hits_df['points'] = track_hits_df['points'].apply(lambda x: str(x))\ntrack_hits_df['muonIDs'] = track_hits_df['muonIDs'].apply(lambda x: str(x))\n\ngraph_hits_df = pd.DataFrame(graph_hits)\ngraph_hits_df.to_csv(graphs_hits_file, index=False)\n\ntrack_hits_df.to_csv(track_hits_file, index=False)\nprint(f'Track hits saved to {track_hits_file}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom collections import Counter\n\n# **🔹 Load Processed Data (for Track Labels)**\nprocessed_data = pd.read_csv('/kaggle/working/processed_data_with_hitid.csv')\nhit_to_track = dict(zip(processed_data['HitID'], processed_data['muonID']))\n\ngraph_dir = \"graphs/\"\ngraph_purities = []\n\n# **🔹 Iterate Over Saved Graphs**\nfor filename in os.listdir(graph_dir):\n    if filename.endswith(\".pkl\"):\n        with open(os.path.join(graph_dir, filename), \"rb\") as f:\n            G = pickle.load(f)\n        \n        # **Get Track Labels for Graph Nodes**\n        hit_ids = list(G.nodes())\n        track_labels = [hit_to_track[hit_id] for hit_id in hit_ids if hit_id in hit_to_track]\n\n        if track_labels:\n            # **Compute Track Counts**\n            track_counts = Counter(track_labels)\n            most_common_track_hits = max(track_counts.values())  # Most frequent track count\n            total_hits = len(track_labels)  # Total hits in the graph\n            \n            purity = most_common_track_hits / total_hits\n            graph_purities.append(purity)\n\n# **🔹 Compute Average Purity**\naverage_purity = (np.mean(graph_purities))*100\nprint(f\"✅ Average Graph Purity: {average_purity:.3f}%\")\n\n# **🔹 Save Purity Results**\npd.DataFrame({\"Graph Purity\": graph_purities}).to_csv(\"graph_purities.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs = pd.read_csv('/kaggle/working/track_hits.csv')\ngraphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs = pd.read_csv('/kaggle/working/track_hits.csv')\ngraphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### edge classification  ###\n\ngraphs_dir = '/kaggle/working/graphs1'\nupdated_graphs_dir = '/kaggle/working/updated_final1'\ntrack_hits_file = '/kaggle/working/track_hits1.csv'\ngraphs_hits_file = '/kaggle/working/graphs_hits.csv'\nos.makedirs(updated_graphs_dir, exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nfeatures = ['pos_z', 'time']\nfor feature in features:\n    min_value = processed_data[feature].min()\n    max_value = processed_data[feature].max()\n    processed_data[feature] = (processed_data[feature] - min_value) / (max_value - min_value)\n\nprint(\"Min-max normalization applied to features:\", features)\n\ndef load_graphs(graphs_dir):\n    graphs = []\n    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pkl')]\n    for graph_file in graph_files:\n        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n            G = pickle.load(f)\n            graphs.append(G)\n    return graphs\n\ngraphs = load_graphs(graphs_dir)\n\n\ndef save_graph_with_scores(graph, updated_graphs_dir, edge_scores, graph_name):\n    graph_file = f'{graph_name}_with_scores.pkl'\n    for (u, v), score in edge_scores.items():\n        if graph.has_edge(u, v):\n            graph[u][v]['score'] = score\n    with open(os.path.join(updated_graphs_dir, graph_file), 'wb') as f:\n        pickle.dump(graph, f)\n\nclass EdgeNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(EdgeNetwork, self).__init__()\n        self.edge_mlp = nn.Sequential(\n            nn.Linear(2 * input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    def forward(self, hi, hj):\n        concatenated = torch.cat((hi, hj), dim=-1)\n        return self.edge_mlp(concatenated)\n\nclass NodeNetwork(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(NodeNetwork, self).__init__()\n        self.node_mlp = nn.Sequential(\n            nn.Linear(3* input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    def forward(self, h_in, s_in, h_out):\n        return self.node_mlp(torch.cat((s_in * h_in, s_in * h_out, h_in), dim=-1)) + h_in\n\nclass GNN(nn.Module):\n    def __init__(self, edge_in_dim, edge_hidden_dim, edge_out_dim,\n                 node_in_dim, node_hidden_dim, node_out_dim, n_iter):\n        super(GNN, self).__init__()\n        self.edge_network = EdgeNetwork(edge_in_dim, edge_hidden_dim, edge_out_dim)\n        self.node_network = NodeNetwork(node_in_dim, node_hidden_dim, node_out_dim)\n        self.n_iter = n_iter\n    def forward(self, node_features, edge_indices):\n        for _ in range(self.n_iter):\n            new_node_features = {}\n            for node_id, neighbors in edge_indices.items():\n                h_in = node_features[node_id]\n                s_in = []\n                for neighbor_id in neighbors:\n                    h_out = node_features[neighbor_id]\n                    s_in.append(self.edge_network(h_in, h_out))\n                if s_in:\n                    s_in = torch.stack(s_in, dim=0)\n                    new_node_features[node_id] = self.node_network(h_in, s_in.mean(dim=0), h_out)\n                else:\n                    new_node_features[node_id] = h_in\n            node_features = new_node_features\n        return node_features\n\nnode_input_dim = 12\nmodel = GNN(edge_in_dim=node_input_dim,\n            edge_hidden_dim=64,\n            edge_out_dim=1,\n            node_in_dim=node_input_dim,\n            node_hidden_dim=64,\n            node_out_dim=node_input_dim,\n            n_iter=2)\n\nmodel.to(device)\n# Load the model parameters to inspect the expected shape\ncheckpoint = torch.load('gnn_model.pth', map_location=device)\n\n# Print the shape of the saved model's first layer weights\nprint(checkpoint['node_network.node_mlp.0.weight'].shape)\n\nmodel.load_state_dict(torch.load(model_gnn, map_location=device))\nmodel.eval()\nprint(\"Model loaded successfully.\")\n\n\nnode_features = {}\nfor index, row in processed_data.iterrows():\n    hit_id = row['HitID']\n    pos_x = row['pos_x']\n    pos_y = row['pos_y']\n    pos_z = row['pos_z']\n    time = row['time']\n    node_features[hit_id] = torch.tensor([pos_x, pos_y, pos_z, time], dtype=torch.float32)\n\nnode_features = {k: v.to(device) for k, v in node_features.items()}\nprint(\"Node features prepared successfully.\")\n\nhitid_to_muonid = processed_data.set_index('HitID')['muonID'].to_dict()\n\ndef create_distance_matrix(edge_scores, num_nodes):\n    rows = []\n    cols = []\n    data = []\n    \n    for (u, v), score in edge_scores.items():\n        if u < num_nodes and v < num_nodes:\n            rows.append(u)\n            cols.append(v)\n            data.append(1 - 2*score)  # distance = 1 - score\n    \n    distance_matrix = csr_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))\n    distance_matrix = (distance_matrix + distance_matrix.T) / 2  # Symmetrize matrix\n    return distance_matrix\n\ndef apply_dbscan(distance_matrix, eps=0.00005):\n    dbscan = DBSCAN(eps=eps, metric='precomputed', min_samples=1)\n    labels = dbscan.fit_predict(distance_matrix)\n    return labels\n\nall_edge_scores = {}\ntrack_hits = {}  # Dictionary to aggregate points and muonIDs by track label\ngraph_hits = []\n\nfor i, G in enumerate(graphs):\n    edge_indices = {node: list(G.neighbors(node)) for node in G.nodes}\n    outputs = model(node_features, edge_indices)\n    edge_scores = {}\n    for node_id, neighbors in edge_indices.items():\n        h_in = outputs[node_id]\n        for neighbor_id in neighbors:\n            h_out = outputs[neighbor_id]\n            edge_score = model.edge_network(h_in, h_out)\n            edge_score = torch.sigmoid(edge_score).item()\n            edge_scores[(node_id, neighbor_id)] = edge_score\n    all_edge_scores[i] = edge_scores\n    save_graph_with_scores(G, updated_graphs_dir, edge_scores, f'graph_{i + 1}')\n    \n    graph_hits.append({\n    'track_number': i + 1,\n    'points': list(G.nodes),\n    'muonIDs': [hitid_to_muonid[hit_id] for hit_id in G.nodes]\n    })\n    \n    num_nodes = len(G.nodes)\n    distance_matrix = create_distance_matrix(all_edge_scores[i], num_nodes)\n    labels = apply_dbscan(distance_matrix, eps=0.1)  # Adjust eps as needed\n    \n    # Aggregate points and muonIDs by track label\n    for j, node in enumerate(G.nodes):\n        label = labels[j]\n        if label not in track_hits:\n            track_hits[label] = {'points': [], 'muonIDs': []}\n        track_hits[label]['points'].append(node)\n        track_hits[label]['muonIDs'].append(hitid_to_muonid.get(node, None))\n\ntrack_hits_list = [{\n    'track_label': label,\n    'points': data['points'],\n    'muonIDs': data['muonIDs']\n} for label, data in track_hits.items()]\n\n# Print the first 5 edge scores\nprint(\"First 5 Edge Scores:\")\nfor i, edge_scores in list(all_edge_scores.items())[:5]:\n    print(f\"Graph {i + 1}:\")\n    for (node_pair, score) in list(edge_scores.items())[:5]:\n        print(f\"  Edge {node_pair}: Score {score}\")\n\n\ntrack_hits_df = pd.DataFrame(track_hits_list)\n\ntrack_hits_df['points'] = track_hits_df['points'].apply(lambda x: str(x))\ntrack_hits_df['muonIDs'] = track_hits_df['muonIDs'].apply(lambda x: str(x))\n\ngraph_hits_df = pd.DataFrame(graph_hits)\ngraph_hits_df.to_csv(graphs_hits_file, index=False)\n\ntrack_hits_df.to_csv(track_hits_file, index=False)\nprint(f'Track hits saved to {track_hits_file}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"graphs = pd.read_csv('/kaggle/working/track_hits.csv')\ngraphs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Manually set the correct CUDA paths\nimport os\n\n# Kaggle's CUDA 11.8 path\nos.environ['CUDA_HOME'] = '/usr/local/cuda-11.8'\nos.environ['PATH'] = '/usr/local/cuda-11.8/bin:' + os.environ['PATH']\nos.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n\n# Verify CUDA paths\n!echo $CUDA_HOME\n!nvcc --version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall torch torch-geometric pyg-lib -y\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}